{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Step 0"
      ],
      "metadata": {
        "id": "9qC5QpqSSdwi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RGsWOzXH9v1",
        "outputId": "d5f2f5ff-5356-42f2-f489-171261107254"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OneDrive_1_01-02-2026.zip  sample_data\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip OneDrive_1_01-02-2026.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "raFg_Q0TPoXg",
        "outputId": "6d30d2f4-a288-4efd-9d6c-4e881d45ebd0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  OneDrive_1_01-02-2026.zip\n",
            " extracting: Test.zip                \n",
            " extracting: Training.zip            \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip Test.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3MtYZ_kPt_E",
        "outputId": "819c0ff8-ac72-40fc-b3af-913656e14bd5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  Test.zip\n",
            "   creating: Test/\n",
            "  inflating: Test/Test.csv           \n",
            "   creating: Test/RGBImages/\n",
            "  inflating: Test/RGBImages/RGB_4.png  \n",
            "  inflating: Test/RGBImages/RGB_5.png  \n",
            "  inflating: Test/RGBImages/RGB_13.png  \n",
            "  inflating: Test/RGBImages/RGB_17.png  \n",
            "  inflating: Test/RGBImages/RGB_24.png  \n",
            "  inflating: Test/RGBImages/RGB_30.png  \n",
            "  inflating: Test/RGBImages/RGB_31.png  \n",
            "  inflating: Test/RGBImages/RGB_36.png  \n",
            "  inflating: Test/RGBImages/RGB_37.png  \n",
            "  inflating: Test/RGBImages/RGB_38.png  \n",
            "  inflating: Test/RGBImages/RGB_41.png  \n",
            "  inflating: Test/RGBImages/RGB_47.png  \n",
            "  inflating: Test/RGBImages/RGB_53.png  \n",
            "  inflating: Test/RGBImages/RGB_61.png  \n",
            "  inflating: Test/RGBImages/RGB_62.png  \n",
            "  inflating: Test/RGBImages/RGB_65.png  \n",
            "  inflating: Test/RGBImages/RGB_69.png  \n",
            "  inflating: Test/RGBImages/RGB_73.png  \n",
            "  inflating: Test/RGBImages/RGB_75.png  \n",
            "  inflating: Test/RGBImages/RGB_83.png  \n",
            "  inflating: Test/RGBImages/RGB_90.png  \n",
            "  inflating: Test/RGBImages/RGB_91.png  \n",
            "  inflating: Test/RGBImages/RGB_94.png  \n",
            "  inflating: Test/RGBImages/RGB_95.png  \n",
            "  inflating: Test/RGBImages/RGB_100.png  \n",
            "  inflating: Test/RGBImages/RGB_103.png  \n",
            "  inflating: Test/RGBImages/RGB_107.png  \n",
            "  inflating: Test/RGBImages/RGB_119.png  \n",
            "  inflating: Test/RGBImages/RGB_123.png  \n",
            "  inflating: Test/RGBImages/RGB_127.png  \n",
            "  inflating: Test/RGBImages/RGB_132.png  \n",
            "  inflating: Test/RGBImages/RGB_137.png  \n",
            "  inflating: Test/RGBImages/RGB_142.png  \n",
            "  inflating: Test/RGBImages/RGB_145.png  \n",
            "  inflating: Test/RGBImages/RGB_157.png  \n",
            "  inflating: Test/RGBImages/RGB_159.png  \n",
            "  inflating: Test/RGBImages/RGB_160.png  \n",
            "  inflating: Test/RGBImages/RGB_166.png  \n",
            "  inflating: Test/RGBImages/RGB_167.png  \n",
            "  inflating: Test/RGBImages/RGB_169.png  \n",
            "  inflating: Test/RGBImages/RGB_174.png  \n",
            "  inflating: Test/RGBImages/RGB_175.png  \n",
            "  inflating: Test/RGBImages/RGB_181.png  \n",
            "  inflating: Test/RGBImages/RGB_186.png  \n",
            "  inflating: Test/RGBImages/RGB_189.png  \n",
            "  inflating: Test/RGBImages/RGB_201.png  \n",
            "  inflating: Test/RGBImages/RGB_205.png  \n",
            "  inflating: Test/RGBImages/RGB_208.png  \n",
            "  inflating: Test/RGBImages/RGB_211.png  \n",
            "  inflating: Test/RGBImages/RGB_220.png  \n",
            "  inflating: Test/RGBImages/RGB_221.png  \n",
            "  inflating: Test/RGBImages/RGB_233.png  \n",
            "  inflating: Test/RGBImages/RGB_238.png  \n",
            "  inflating: Test/RGBImages/RGB_239.png  \n",
            "  inflating: Test/RGBImages/RGB_242.png  \n",
            "  inflating: Test/RGBImages/RGB_257.png  \n",
            "  inflating: Test/RGBImages/RGB_261.png  \n",
            "  inflating: Test/RGBImages/RGB_268.png  \n",
            "  inflating: Test/RGBImages/RGB_283.png  \n",
            "  inflating: Test/RGBImages/RGB_284.png  \n",
            "  inflating: Test/RGBImages/RGB_288.png  \n",
            "  inflating: Test/RGBImages/RGB_292.png  \n",
            "  inflating: Test/RGBImages/RGB_294.png  \n",
            "  inflating: Test/RGBImages/RGB_306.png  \n",
            "  inflating: Test/RGBImages/RGB_312.png  \n",
            "  inflating: Test/RGBImages/RGB_314.png  \n",
            "  inflating: Test/RGBImages/RGB_317.png  \n",
            "  inflating: Test/RGBImages/RGB_320.png  \n",
            "  inflating: Test/RGBImages/RGB_326.png  \n",
            "  inflating: Test/RGBImages/RGB_344.png  \n",
            "  inflating: Test/RGBImages/RGB_349.png  \n",
            "  inflating: Test/RGBImages/RGB_354.png  \n",
            "  inflating: Test/RGBImages/RGB_367.png  \n",
            "  inflating: Test/RGBImages/RGB_373.png  \n",
            "  inflating: Test/RGBImages/RGB_385.png  \n",
            "  inflating: Test/RGBImages/RGB_388.png  \n",
            "   creating: Test/DepthImages/\n",
            "  inflating: Test/DepthImages/Depth_4.png  \n",
            "  inflating: Test/DepthImages/Depth_5.png  \n",
            "  inflating: Test/DepthImages/Depth_13.png  \n",
            "  inflating: Test/DepthImages/Depth_17.png  \n",
            "  inflating: Test/DepthImages/Depth_24.png  \n",
            "  inflating: Test/DepthImages/Depth_30.png  \n",
            "  inflating: Test/DepthImages/Depth_31.png  \n",
            "  inflating: Test/DepthImages/Depth_36.png  \n",
            "  inflating: Test/DepthImages/Depth_37.png  \n",
            "  inflating: Test/DepthImages/Depth_38.png  \n",
            "  inflating: Test/DepthImages/Depth_41.png  \n",
            "  inflating: Test/DepthImages/Depth_47.png  \n",
            "  inflating: Test/DepthImages/Depth_53.png  \n",
            "  inflating: Test/DepthImages/Depth_61.png  \n",
            "  inflating: Test/DepthImages/Depth_62.png  \n",
            "  inflating: Test/DepthImages/Depth_65.png  \n",
            "  inflating: Test/DepthImages/Depth_69.png  \n",
            "  inflating: Test/DepthImages/Depth_73.png  \n",
            "  inflating: Test/DepthImages/Depth_75.png  \n",
            "  inflating: Test/DepthImages/Depth_83.png  \n",
            "  inflating: Test/DepthImages/Depth_90.png  \n",
            "  inflating: Test/DepthImages/Depth_91.png  \n",
            "  inflating: Test/DepthImages/Depth_94.png  \n",
            "  inflating: Test/DepthImages/Depth_95.png  \n",
            "  inflating: Test/DepthImages/Depth_100.png  \n",
            "  inflating: Test/DepthImages/Depth_103.png  \n",
            "  inflating: Test/DepthImages/Depth_107.png  \n",
            "  inflating: Test/DepthImages/Depth_119.png  \n",
            "  inflating: Test/DepthImages/Depth_123.png  \n",
            "  inflating: Test/DepthImages/Depth_127.png  \n",
            "  inflating: Test/DepthImages/Depth_132.png  \n",
            "  inflating: Test/DepthImages/Depth_137.png  \n",
            "  inflating: Test/DepthImages/Depth_142.png  \n",
            "  inflating: Test/DepthImages/Depth_145.png  \n",
            "  inflating: Test/DepthImages/Depth_157.png  \n",
            "  inflating: Test/DepthImages/Depth_159.png  \n",
            "  inflating: Test/DepthImages/Depth_160.png  \n",
            "  inflating: Test/DepthImages/Depth_166.png  \n",
            "  inflating: Test/DepthImages/Depth_167.png  \n",
            "  inflating: Test/DepthImages/Depth_169.png  \n",
            "  inflating: Test/DepthImages/Depth_174.png  \n",
            "  inflating: Test/DepthImages/Depth_175.png  \n",
            "  inflating: Test/DepthImages/Depth_181.png  \n",
            "  inflating: Test/DepthImages/Depth_186.png  \n",
            "  inflating: Test/DepthImages/Depth_189.png  \n",
            "  inflating: Test/DepthImages/Depth_201.png  \n",
            "  inflating: Test/DepthImages/Depth_205.png  \n",
            "  inflating: Test/DepthImages/Depth_208.png  \n",
            "  inflating: Test/DepthImages/Depth_211.png  \n",
            "  inflating: Test/DepthImages/Depth_220.png  \n",
            "  inflating: Test/DepthImages/Depth_221.png  \n",
            "  inflating: Test/DepthImages/Depth_233.png  \n",
            "  inflating: Test/DepthImages/Depth_238.png  \n",
            "  inflating: Test/DepthImages/Depth_239.png  \n",
            "  inflating: Test/DepthImages/Depth_242.png  \n",
            "  inflating: Test/DepthImages/Depth_257.png  \n",
            "  inflating: Test/DepthImages/Depth_261.png  \n",
            "  inflating: Test/DepthImages/Depth_268.png  \n",
            "  inflating: Test/DepthImages/Depth_283.png  \n",
            "  inflating: Test/DepthImages/Depth_284.png  \n",
            "  inflating: Test/DepthImages/Depth_288.png  \n",
            "  inflating: Test/DepthImages/Depth_292.png  \n",
            "  inflating: Test/DepthImages/Depth_294.png  \n",
            "  inflating: Test/DepthImages/Depth_306.png  \n",
            "  inflating: Test/DepthImages/Depth_312.png  \n",
            "  inflating: Test/DepthImages/Depth_314.png  \n",
            "  inflating: Test/DepthImages/Depth_317.png  \n",
            "  inflating: Test/DepthImages/Depth_320.png  \n",
            "  inflating: Test/DepthImages/Depth_326.png  \n",
            "  inflating: Test/DepthImages/Depth_344.png  \n",
            "  inflating: Test/DepthImages/Depth_349.png  \n",
            "  inflating: Test/DepthImages/Depth_354.png  \n",
            "  inflating: Test/DepthImages/Depth_367.png  \n",
            "  inflating: Test/DepthImages/Depth_373.png  \n",
            "  inflating: Test/DepthImages/Depth_385.png  \n",
            "  inflating: Test/DepthImages/Depth_388.png  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip Training.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tin7i8KIP1P_",
        "outputId": "a3266866-7fef-417b-ada5-7dfcb586f309"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  Training.zip\n",
            "   creating: Training/\n",
            "   creating: Training/RGBImages/\n",
            "  inflating: Training/RGBImages/RGB_1.png  \n",
            "  inflating: Training/RGBImages/RGB_2.png  \n",
            "  inflating: Training/RGBImages/RGB_3.png  \n",
            "  inflating: Training/RGBImages/RGB_6.png  \n",
            "  inflating: Training/RGBImages/RGB_8.png  \n",
            "  inflating: Training/RGBImages/RGB_9.png  \n",
            "  inflating: Training/RGBImages/RGB_11.png  \n",
            "  inflating: Training/RGBImages/RGB_14.png  \n",
            "  inflating: Training/RGBImages/RGB_15.png  \n",
            "  inflating: Training/RGBImages/RGB_16.png  \n",
            "  inflating: Training/RGBImages/RGB_18.png  \n",
            "  inflating: Training/RGBImages/RGB_19.png  \n",
            "  inflating: Training/RGBImages/RGB_20.png  \n",
            "  inflating: Training/RGBImages/RGB_21.png  \n",
            "  inflating: Training/RGBImages/RGB_22.png  \n",
            "  inflating: Training/RGBImages/RGB_25.png  \n",
            "  inflating: Training/RGBImages/RGB_27.png  \n",
            "  inflating: Training/RGBImages/RGB_28.png  \n",
            "  inflating: Training/RGBImages/RGB_29.png  \n",
            "  inflating: Training/RGBImages/RGB_32.png  \n",
            "  inflating: Training/RGBImages/RGB_34.png  \n",
            "  inflating: Training/RGBImages/RGB_35.png  \n",
            "  inflating: Training/RGBImages/RGB_39.png  \n",
            "  inflating: Training/RGBImages/RGB_40.png  \n",
            "  inflating: Training/RGBImages/RGB_42.png  \n",
            "  inflating: Training/RGBImages/RGB_43.png  \n",
            "  inflating: Training/RGBImages/RGB_44.png  \n",
            "  inflating: Training/RGBImages/RGB_48.png  \n",
            "  inflating: Training/RGBImages/RGB_49.png  \n",
            "  inflating: Training/RGBImages/RGB_54.png  \n",
            "  inflating: Training/RGBImages/RGB_55.png  \n",
            "  inflating: Training/RGBImages/RGB_56.png  \n",
            "  inflating: Training/RGBImages/RGB_57.png  \n",
            "  inflating: Training/RGBImages/RGB_58.png  \n",
            "  inflating: Training/RGBImages/RGB_59.png  \n",
            "  inflating: Training/RGBImages/RGB_60.png  \n",
            "  inflating: Training/RGBImages/RGB_63.png  \n",
            "  inflating: Training/RGBImages/RGB_64.png  \n",
            "  inflating: Training/RGBImages/RGB_66.png  \n",
            "  inflating: Training/RGBImages/RGB_68.png  \n",
            "  inflating: Training/RGBImages/RGB_70.png  \n",
            "  inflating: Training/RGBImages/RGB_71.png  \n",
            "  inflating: Training/RGBImages/RGB_72.png  \n",
            "  inflating: Training/RGBImages/RGB_74.png  \n",
            "  inflating: Training/RGBImages/RGB_76.png  \n",
            "  inflating: Training/RGBImages/RGB_77.png  \n",
            "  inflating: Training/RGBImages/RGB_79.png  \n",
            "  inflating: Training/RGBImages/RGB_81.png  \n",
            "  inflating: Training/RGBImages/RGB_82.png  \n",
            "  inflating: Training/RGBImages/RGB_84.png  \n",
            "  inflating: Training/RGBImages/RGB_86.png  \n",
            "  inflating: Training/RGBImages/RGB_87.png  \n",
            "  inflating: Training/RGBImages/RGB_88.png  \n",
            "  inflating: Training/RGBImages/RGB_89.png  \n",
            "  inflating: Training/RGBImages/RGB_93.png  \n",
            "  inflating: Training/RGBImages/RGB_97.png  \n",
            "  inflating: Training/RGBImages/RGB_98.png  \n",
            "  inflating: Training/RGBImages/RGB_99.png  \n",
            "  inflating: Training/RGBImages/RGB_101.png  \n",
            "  inflating: Training/RGBImages/RGB_102.png  \n",
            "  inflating: Training/RGBImages/RGB_104.png  \n",
            "  inflating: Training/RGBImages/RGB_105.png  \n",
            "  inflating: Training/RGBImages/RGB_106.png  \n",
            "  inflating: Training/RGBImages/RGB_108.png  \n",
            "  inflating: Training/RGBImages/RGB_110.png  \n",
            "  inflating: Training/RGBImages/RGB_113.png  \n",
            "  inflating: Training/RGBImages/RGB_114.png  \n",
            "  inflating: Training/RGBImages/RGB_118.png  \n",
            "  inflating: Training/RGBImages/RGB_120.png  \n",
            "  inflating: Training/RGBImages/RGB_122.png  \n",
            "  inflating: Training/RGBImages/RGB_125.png  \n",
            "  inflating: Training/RGBImages/RGB_126.png  \n",
            "  inflating: Training/RGBImages/RGB_128.png  \n",
            "  inflating: Training/RGBImages/RGB_129.png  \n",
            "  inflating: Training/RGBImages/RGB_131.png  \n",
            "  inflating: Training/RGBImages/RGB_133.png  \n",
            "  inflating: Training/RGBImages/RGB_134.png  \n",
            "  inflating: Training/RGBImages/RGB_135.png  \n",
            "  inflating: Training/RGBImages/RGB_136.png  \n",
            "  inflating: Training/RGBImages/RGB_138.png  \n",
            "  inflating: Training/RGBImages/RGB_139.png  \n",
            "  inflating: Training/RGBImages/RGB_140.png  \n",
            "  inflating: Training/RGBImages/RGB_143.png  \n",
            "  inflating: Training/RGBImages/RGB_144.png  \n",
            "  inflating: Training/RGBImages/RGB_146.png  \n",
            "  inflating: Training/RGBImages/RGB_148.png  \n",
            "  inflating: Training/RGBImages/RGB_149.png  \n",
            "  inflating: Training/RGBImages/RGB_150.png  \n",
            "  inflating: Training/RGBImages/RGB_152.png  \n",
            "  inflating: Training/RGBImages/RGB_153.png  \n",
            "  inflating: Training/RGBImages/RGB_154.png  \n",
            "  inflating: Training/RGBImages/RGB_156.png  \n",
            "  inflating: Training/RGBImages/RGB_161.png  \n",
            "  inflating: Training/RGBImages/RGB_162.png  \n",
            "  inflating: Training/RGBImages/RGB_163.png  \n",
            "  inflating: Training/RGBImages/RGB_164.png  \n",
            "  inflating: Training/RGBImages/RGB_165.png  \n",
            "  inflating: Training/RGBImages/RGB_168.png  \n",
            "  inflating: Training/RGBImages/RGB_170.png  \n",
            "  inflating: Training/RGBImages/RGB_171.png  \n",
            "  inflating: Training/RGBImages/RGB_172.png  \n",
            "  inflating: Training/RGBImages/RGB_173.png  \n",
            "  inflating: Training/RGBImages/RGB_176.png  \n",
            "  inflating: Training/RGBImages/RGB_177.png  \n",
            "  inflating: Training/RGBImages/RGB_179.png  \n",
            "  inflating: Training/RGBImages/RGB_180.png  \n",
            "  inflating: Training/RGBImages/RGB_182.png  \n",
            "  inflating: Training/RGBImages/RGB_183.png  \n",
            "  inflating: Training/RGBImages/RGB_184.png  \n",
            "  inflating: Training/RGBImages/RGB_187.png  \n",
            "  inflating: Training/RGBImages/RGB_188.png  \n",
            "  inflating: Training/RGBImages/RGB_190.png  \n",
            "  inflating: Training/RGBImages/RGB_191.png  \n",
            "  inflating: Training/RGBImages/RGB_193.png  \n",
            "  inflating: Training/RGBImages/RGB_194.png  \n",
            "  inflating: Training/RGBImages/RGB_195.png  \n",
            "  inflating: Training/RGBImages/RGB_196.png  \n",
            "  inflating: Training/RGBImages/RGB_197.png  \n",
            "  inflating: Training/RGBImages/RGB_202.png  \n",
            "  inflating: Training/RGBImages/RGB_203.png  \n",
            "  inflating: Training/RGBImages/RGB_204.png  \n",
            "  inflating: Training/RGBImages/RGB_206.png  \n",
            "  inflating: Training/RGBImages/RGB_207.png  \n",
            "  inflating: Training/RGBImages/RGB_210.png  \n",
            "  inflating: Training/RGBImages/RGB_212.png  \n",
            "  inflating: Training/RGBImages/RGB_214.png  \n",
            "  inflating: Training/RGBImages/RGB_215.png  \n",
            "  inflating: Training/RGBImages/RGB_216.png  \n",
            "  inflating: Training/RGBImages/RGB_217.png  \n",
            "  inflating: Training/RGBImages/RGB_218.png  \n",
            "  inflating: Training/RGBImages/RGB_223.png  \n",
            "  inflating: Training/RGBImages/RGB_225.png  \n",
            "  inflating: Training/RGBImages/RGB_226.png  \n",
            "  inflating: Training/RGBImages/RGB_227.png  \n",
            "  inflating: Training/RGBImages/RGB_229.png  \n",
            "  inflating: Training/RGBImages/RGB_230.png  \n",
            "  inflating: Training/RGBImages/RGB_234.png  \n",
            "  inflating: Training/RGBImages/RGB_235.png  \n",
            "  inflating: Training/RGBImages/RGB_236.png  \n",
            "  inflating: Training/RGBImages/RGB_237.png  \n",
            "  inflating: Training/RGBImages/RGB_240.png  \n",
            "  inflating: Training/RGBImages/RGB_241.png  \n",
            "  inflating: Training/RGBImages/RGB_243.png  \n",
            "  inflating: Training/RGBImages/RGB_244.png  \n",
            "  inflating: Training/RGBImages/RGB_245.png  \n",
            "  inflating: Training/RGBImages/RGB_246.png  \n",
            "  inflating: Training/RGBImages/RGB_249.png  \n",
            "  inflating: Training/RGBImages/RGB_250.png  \n",
            "  inflating: Training/RGBImages/RGB_251.png  \n",
            "  inflating: Training/RGBImages/RGB_252.png  \n",
            "  inflating: Training/RGBImages/RGB_253.png  \n",
            "  inflating: Training/RGBImages/RGB_255.png  \n",
            "  inflating: Training/RGBImages/RGB_256.png  \n",
            "  inflating: Training/RGBImages/RGB_258.png  \n",
            "  inflating: Training/RGBImages/RGB_260.png  \n",
            "  inflating: Training/RGBImages/RGB_262.png  \n",
            "  inflating: Training/RGBImages/RGB_263.png  \n",
            "  inflating: Training/RGBImages/RGB_264.png  \n",
            "  inflating: Training/RGBImages/RGB_265.png  \n",
            "  inflating: Training/RGBImages/RGB_266.png  \n",
            "  inflating: Training/RGBImages/RGB_267.png  \n",
            "  inflating: Training/RGBImages/RGB_269.png  \n",
            "  inflating: Training/RGBImages/RGB_270.png  \n",
            "  inflating: Training/RGBImages/RGB_271.png  \n",
            "  inflating: Training/RGBImages/RGB_272.png  \n",
            "  inflating: Training/RGBImages/RGB_273.png  \n",
            "  inflating: Training/RGBImages/RGB_275.png  \n",
            "  inflating: Training/RGBImages/RGB_276.png  \n",
            "  inflating: Training/RGBImages/RGB_278.png  \n",
            "  inflating: Training/RGBImages/RGB_279.png  \n",
            "  inflating: Training/RGBImages/RGB_282.png  \n",
            "  inflating: Training/RGBImages/RGB_285.png  \n",
            "  inflating: Training/RGBImages/RGB_286.png  \n",
            "  inflating: Training/RGBImages/RGB_287.png  \n",
            "  inflating: Training/RGBImages/RGB_289.png  \n",
            "  inflating: Training/RGBImages/RGB_290.png  \n",
            "  inflating: Training/RGBImages/RGB_293.png  \n",
            "  inflating: Training/RGBImages/RGB_297.png  \n",
            "  inflating: Training/RGBImages/RGB_298.png  \n",
            "  inflating: Training/RGBImages/RGB_299.png  \n",
            "  inflating: Training/RGBImages/RGB_300.png  \n",
            "  inflating: Training/RGBImages/RGB_301.png  \n",
            "  inflating: Training/RGBImages/RGB_302.png  \n",
            "  inflating: Training/RGBImages/RGB_310.png  \n",
            "  inflating: Training/RGBImages/RGB_311.png  \n",
            "  inflating: Training/RGBImages/RGB_313.png  \n",
            "  inflating: Training/RGBImages/RGB_315.png  \n",
            "  inflating: Training/RGBImages/RGB_316.png  \n",
            "  inflating: Training/RGBImages/RGB_318.png  \n",
            "  inflating: Training/RGBImages/RGB_319.png  \n",
            "  inflating: Training/RGBImages/RGB_321.png  \n",
            "  inflating: Training/RGBImages/RGB_323.png  \n",
            "  inflating: Training/RGBImages/RGB_325.png  \n",
            "  inflating: Training/RGBImages/RGB_329.png  \n",
            "  inflating: Training/RGBImages/RGB_330.png  \n",
            "  inflating: Training/RGBImages/RGB_334.png  \n",
            "  inflating: Training/RGBImages/RGB_335.png  \n",
            "  inflating: Training/RGBImages/RGB_337.png  \n",
            "  inflating: Training/RGBImages/RGB_338.png  \n",
            "  inflating: Training/RGBImages/RGB_341.png  \n",
            "  inflating: Training/RGBImages/RGB_342.png  \n",
            "  inflating: Training/RGBImages/RGB_345.png  \n",
            "  inflating: Training/RGBImages/RGB_346.png  \n",
            "  inflating: Training/RGBImages/RGB_347.png  \n",
            "  inflating: Training/RGBImages/RGB_348.png  \n",
            "  inflating: Training/RGBImages/RGB_350.png  \n",
            "  inflating: Training/RGBImages/RGB_352.png  \n",
            "  inflating: Training/RGBImages/RGB_353.png  \n",
            "  inflating: Training/RGBImages/RGB_356.png  \n",
            "  inflating: Training/RGBImages/RGB_357.png  \n",
            "  inflating: Training/RGBImages/RGB_358.png  \n",
            "  inflating: Training/RGBImages/RGB_359.png  \n",
            "  inflating: Training/RGBImages/RGB_361.png  \n",
            "  inflating: Training/RGBImages/RGB_363.png  \n",
            "  inflating: Training/RGBImages/RGB_366.png  \n",
            "  inflating: Training/RGBImages/RGB_368.png  \n",
            "  inflating: Training/RGBImages/RGB_369.png  \n",
            "  inflating: Training/RGBImages/RGB_371.png  \n",
            "  inflating: Training/RGBImages/RGB_372.png  \n",
            "  inflating: Training/RGBImages/RGB_375.png  \n",
            "  inflating: Training/RGBImages/RGB_376.png  \n",
            "  inflating: Training/RGBImages/RGB_377.png  \n",
            "  inflating: Training/RGBImages/RGB_378.png  \n",
            "  inflating: Training/RGBImages/RGB_379.png  \n",
            "  inflating: Training/RGBImages/RGB_380.png  \n",
            "  inflating: Training/RGBImages/RGB_381.png  \n",
            "  inflating: Training/RGBImages/RGB_382.png  \n",
            "  inflating: Training/RGBImages/RGB_387.png  \n",
            "  inflating: Training/RGBImages/RGB_389.png  \n",
            "  inflating: Training/RGBImages/RGB_390.png  \n",
            "  inflating: Training/Train.csv      \n",
            "   creating: Training/DepthImages/\n",
            "  inflating: Training/DepthImages/Depth_1.png  \n",
            "  inflating: Training/DepthImages/Depth_2.png  \n",
            "  inflating: Training/DepthImages/Depth_3.png  \n",
            "  inflating: Training/DepthImages/Depth_6.png  \n",
            "  inflating: Training/DepthImages/Depth_8.png  \n",
            "  inflating: Training/DepthImages/Depth_9.png  \n",
            "  inflating: Training/DepthImages/Depth_11.png  \n",
            "  inflating: Training/DepthImages/Depth_14.png  \n",
            "  inflating: Training/DepthImages/Depth_15.png  \n",
            "  inflating: Training/DepthImages/Depth_16.png  \n",
            "  inflating: Training/DepthImages/Depth_18.png  \n",
            "  inflating: Training/DepthImages/Depth_19.png  \n",
            "  inflating: Training/DepthImages/Depth_20.png  \n",
            "  inflating: Training/DepthImages/Depth_21.png  \n",
            "  inflating: Training/DepthImages/Depth_22.png  \n",
            "  inflating: Training/DepthImages/Depth_25.png  \n",
            "  inflating: Training/DepthImages/Depth_27.png  \n",
            "  inflating: Training/DepthImages/Depth_28.png  \n",
            "  inflating: Training/DepthImages/Depth_29.png  \n",
            "  inflating: Training/DepthImages/Depth_32.png  \n",
            "  inflating: Training/DepthImages/Depth_34.png  \n",
            "  inflating: Training/DepthImages/Depth_35.png  \n",
            "  inflating: Training/DepthImages/Depth_39.png  \n",
            "  inflating: Training/DepthImages/Depth_40.png  \n",
            "  inflating: Training/DepthImages/Depth_42.png  \n",
            "  inflating: Training/DepthImages/Depth_43.png  \n",
            "  inflating: Training/DepthImages/Depth_44.png  \n",
            "  inflating: Training/DepthImages/Depth_48.png  \n",
            "  inflating: Training/DepthImages/Depth_49.png  \n",
            "  inflating: Training/DepthImages/Depth_54.png  \n",
            "  inflating: Training/DepthImages/Depth_55.png  \n",
            "  inflating: Training/DepthImages/Depth_56.png  \n",
            "  inflating: Training/DepthImages/Depth_57.png  \n",
            "  inflating: Training/DepthImages/Depth_58.png  \n",
            "  inflating: Training/DepthImages/Depth_59.png  \n",
            "  inflating: Training/DepthImages/Depth_60.png  \n",
            "  inflating: Training/DepthImages/Depth_63.png  \n",
            "  inflating: Training/DepthImages/Depth_64.png  \n",
            "  inflating: Training/DepthImages/Depth_66.png  \n",
            "  inflating: Training/DepthImages/Depth_68.png  \n",
            "  inflating: Training/DepthImages/Depth_70.png  \n",
            "  inflating: Training/DepthImages/Depth_71.png  \n",
            "  inflating: Training/DepthImages/Depth_72.png  \n",
            "  inflating: Training/DepthImages/Depth_74.png  \n",
            "  inflating: Training/DepthImages/Depth_76.png  \n",
            "  inflating: Training/DepthImages/Depth_77.png  \n",
            "  inflating: Training/DepthImages/Depth_79.png  \n",
            "  inflating: Training/DepthImages/Depth_81.png  \n",
            "  inflating: Training/DepthImages/Depth_82.png  \n",
            "  inflating: Training/DepthImages/Depth_84.png  \n",
            "  inflating: Training/DepthImages/Depth_86.png  \n",
            "  inflating: Training/DepthImages/Depth_87.png  \n",
            "  inflating: Training/DepthImages/Depth_88.png  \n",
            "  inflating: Training/DepthImages/Depth_89.png  \n",
            "  inflating: Training/DepthImages/Depth_93.png  \n",
            "  inflating: Training/DepthImages/Depth_97.png  \n",
            "  inflating: Training/DepthImages/Depth_98.png  \n",
            "  inflating: Training/DepthImages/Depth_99.png  \n",
            "  inflating: Training/DepthImages/Depth_101.png  \n",
            "  inflating: Training/DepthImages/Depth_102.png  \n",
            "  inflating: Training/DepthImages/Depth_104.png  \n",
            "  inflating: Training/DepthImages/Depth_105.png  \n",
            "  inflating: Training/DepthImages/Depth_106.png  \n",
            "  inflating: Training/DepthImages/Depth_108.png  \n",
            "  inflating: Training/DepthImages/Depth_110.png  \n",
            "  inflating: Training/DepthImages/Depth_113.png  \n",
            "  inflating: Training/DepthImages/Depth_114.png  \n",
            "  inflating: Training/DepthImages/Depth_118.png  \n",
            "  inflating: Training/DepthImages/Depth_120.png  \n",
            "  inflating: Training/DepthImages/Depth_122.png  \n",
            "  inflating: Training/DepthImages/Depth_125.png  \n",
            "  inflating: Training/DepthImages/Depth_126.png  \n",
            "  inflating: Training/DepthImages/Depth_128.png  \n",
            "  inflating: Training/DepthImages/Depth_129.png  \n",
            "  inflating: Training/DepthImages/Depth_131.png  \n",
            "  inflating: Training/DepthImages/Depth_133.png  \n",
            "  inflating: Training/DepthImages/Depth_134.png  \n",
            "  inflating: Training/DepthImages/Depth_135.png  \n",
            "  inflating: Training/DepthImages/Depth_136.png  \n",
            "  inflating: Training/DepthImages/Depth_138.png  \n",
            "  inflating: Training/DepthImages/Depth_139.png  \n",
            "  inflating: Training/DepthImages/Depth_140.png  \n",
            "  inflating: Training/DepthImages/Depth_143.png  \n",
            "  inflating: Training/DepthImages/Depth_144.png  \n",
            "  inflating: Training/DepthImages/Depth_146.png  \n",
            "  inflating: Training/DepthImages/Depth_148.png  \n",
            "  inflating: Training/DepthImages/Depth_149.png  \n",
            "  inflating: Training/DepthImages/Depth_150.png  \n",
            "  inflating: Training/DepthImages/Depth_152.png  \n",
            "  inflating: Training/DepthImages/Depth_153.png  \n",
            "  inflating: Training/DepthImages/Depth_154.png  \n",
            "  inflating: Training/DepthImages/Depth_156.png  \n",
            "  inflating: Training/DepthImages/Depth_161.png  \n",
            "  inflating: Training/DepthImages/Depth_162.png  \n",
            "  inflating: Training/DepthImages/Depth_163.png  \n",
            "  inflating: Training/DepthImages/Depth_164.png  \n",
            "  inflating: Training/DepthImages/Depth_165.png  \n",
            "  inflating: Training/DepthImages/Depth_168.png  \n",
            "  inflating: Training/DepthImages/Depth_170.png  \n",
            "  inflating: Training/DepthImages/Depth_171.png  \n",
            "  inflating: Training/DepthImages/Depth_172.png  \n",
            "  inflating: Training/DepthImages/Depth_173.png  \n",
            "  inflating: Training/DepthImages/Depth_176.png  \n",
            "  inflating: Training/DepthImages/Depth_177.png  \n",
            "  inflating: Training/DepthImages/Depth_179.png  \n",
            "  inflating: Training/DepthImages/Depth_180.png  \n",
            "  inflating: Training/DepthImages/Depth_182.png  \n",
            "  inflating: Training/DepthImages/Depth_183.png  \n",
            "  inflating: Training/DepthImages/Depth_184.png  \n",
            "  inflating: Training/DepthImages/Depth_187.png  \n",
            "  inflating: Training/DepthImages/Depth_188.png  \n",
            "  inflating: Training/DepthImages/Depth_190.png  \n",
            "  inflating: Training/DepthImages/Depth_191.png  \n",
            "  inflating: Training/DepthImages/Depth_193.png  \n",
            "  inflating: Training/DepthImages/Depth_194.png  \n",
            "  inflating: Training/DepthImages/Depth_195.png  \n",
            "  inflating: Training/DepthImages/Depth_196.png  \n",
            "  inflating: Training/DepthImages/Depth_197.png  \n",
            "  inflating: Training/DepthImages/Depth_202.png  \n",
            "  inflating: Training/DepthImages/Depth_203.png  \n",
            "  inflating: Training/DepthImages/Depth_204.png  \n",
            "  inflating: Training/DepthImages/Depth_206.png  \n",
            "  inflating: Training/DepthImages/Depth_207.png  \n",
            "  inflating: Training/DepthImages/Depth_210.png  \n",
            "  inflating: Training/DepthImages/Depth_212.png  \n",
            "  inflating: Training/DepthImages/Depth_214.png  \n",
            "  inflating: Training/DepthImages/Depth_215.png  \n",
            "  inflating: Training/DepthImages/Depth_216.png  \n",
            "  inflating: Training/DepthImages/Depth_217.png  \n",
            "  inflating: Training/DepthImages/Depth_218.png  \n",
            "  inflating: Training/DepthImages/Depth_223.png  \n",
            "  inflating: Training/DepthImages/Depth_225.png  \n",
            "  inflating: Training/DepthImages/Depth_226.png  \n",
            "  inflating: Training/DepthImages/Depth_227.png  \n",
            "  inflating: Training/DepthImages/Depth_229.png  \n",
            "  inflating: Training/DepthImages/Depth_230.png  \n",
            "  inflating: Training/DepthImages/Depth_234.png  \n",
            "  inflating: Training/DepthImages/Depth_235.png  \n",
            "  inflating: Training/DepthImages/Depth_236.png  \n",
            "  inflating: Training/DepthImages/Depth_237.png  \n",
            "  inflating: Training/DepthImages/Depth_240.png  \n",
            "  inflating: Training/DepthImages/Depth_241.png  \n",
            "  inflating: Training/DepthImages/Depth_243.png  \n",
            "  inflating: Training/DepthImages/Depth_244.png  \n",
            "  inflating: Training/DepthImages/Depth_245.png  \n",
            "  inflating: Training/DepthImages/Depth_246.png  \n",
            "  inflating: Training/DepthImages/Depth_249.png  \n",
            "  inflating: Training/DepthImages/Depth_250.png  \n",
            "  inflating: Training/DepthImages/Depth_251.png  \n",
            "  inflating: Training/DepthImages/Depth_252.png  \n",
            "  inflating: Training/DepthImages/Depth_253.png  \n",
            "  inflating: Training/DepthImages/Depth_255.png  \n",
            "  inflating: Training/DepthImages/Depth_256.png  \n",
            "  inflating: Training/DepthImages/Depth_258.png  \n",
            "  inflating: Training/DepthImages/Depth_260.png  \n",
            "  inflating: Training/DepthImages/Depth_262.png  \n",
            "  inflating: Training/DepthImages/Depth_263.png  \n",
            "  inflating: Training/DepthImages/Depth_264.png  \n",
            "  inflating: Training/DepthImages/Depth_265.png  \n",
            "  inflating: Training/DepthImages/Depth_266.png  \n",
            "  inflating: Training/DepthImages/Depth_267.png  \n",
            "  inflating: Training/DepthImages/Depth_269.png  \n",
            "  inflating: Training/DepthImages/Depth_270.png  \n",
            "  inflating: Training/DepthImages/Depth_271.png  \n",
            "  inflating: Training/DepthImages/Depth_272.png  \n",
            "  inflating: Training/DepthImages/Depth_273.png  \n",
            "  inflating: Training/DepthImages/Depth_275.png  \n",
            "  inflating: Training/DepthImages/Depth_276.png  \n",
            "  inflating: Training/DepthImages/Depth_278.png  \n",
            "  inflating: Training/DepthImages/Depth_279.png  \n",
            "  inflating: Training/DepthImages/Depth_282.png  \n",
            "  inflating: Training/DepthImages/Depth_285.png  \n",
            "  inflating: Training/DepthImages/Depth_286.png  \n",
            "  inflating: Training/DepthImages/Depth_287.png  \n",
            "  inflating: Training/DepthImages/Depth_289.png  \n",
            "  inflating: Training/DepthImages/Depth_290.png  \n",
            "  inflating: Training/DepthImages/Depth_293.png  \n",
            "  inflating: Training/DepthImages/Depth_297.png  \n",
            "  inflating: Training/DepthImages/Depth_298.png  \n",
            "  inflating: Training/DepthImages/Depth_299.png  \n",
            "  inflating: Training/DepthImages/Depth_300.png  \n",
            "  inflating: Training/DepthImages/Depth_301.png  \n",
            "  inflating: Training/DepthImages/Depth_302.png  \n",
            "  inflating: Training/DepthImages/Depth_310.png  \n",
            "  inflating: Training/DepthImages/Depth_311.png  \n",
            "  inflating: Training/DepthImages/Depth_313.png  \n",
            "  inflating: Training/DepthImages/Depth_315.png  \n",
            "  inflating: Training/DepthImages/Depth_316.png  \n",
            "  inflating: Training/DepthImages/Depth_318.png  \n",
            "  inflating: Training/DepthImages/Depth_319.png  \n",
            "  inflating: Training/DepthImages/Depth_321.png  \n",
            "  inflating: Training/DepthImages/Depth_323.png  \n",
            "  inflating: Training/DepthImages/Depth_325.png  \n",
            "  inflating: Training/DepthImages/Depth_329.png  \n",
            "  inflating: Training/DepthImages/Depth_330.png  \n",
            "  inflating: Training/DepthImages/Depth_332.png  \n",
            "  inflating: Training/DepthImages/Depth_334.png  \n",
            "  inflating: Training/DepthImages/Depth_335.png  \n",
            "  inflating: Training/DepthImages/Depth_337.png  \n",
            "  inflating: Training/DepthImages/Depth_338.png  \n",
            "  inflating: Training/DepthImages/Depth_341.png  \n",
            "  inflating: Training/DepthImages/Depth_342.png  \n",
            "  inflating: Training/DepthImages/Depth_345.png  \n",
            "  inflating: Training/DepthImages/Depth_346.png  \n",
            "  inflating: Training/DepthImages/Depth_347.png  \n",
            "  inflating: Training/DepthImages/Depth_348.png  \n",
            "  inflating: Training/DepthImages/Depth_350.png  \n",
            "  inflating: Training/DepthImages/Depth_352.png  \n",
            "  inflating: Training/DepthImages/Depth_353.png  \n",
            "  inflating: Training/DepthImages/Depth_356.png  \n",
            "  inflating: Training/DepthImages/Depth_357.png  \n",
            "  inflating: Training/DepthImages/Depth_358.png  \n",
            "  inflating: Training/DepthImages/Depth_359.png  \n",
            "  inflating: Training/DepthImages/Depth_361.png  \n",
            "  inflating: Training/DepthImages/Depth_363.png  \n",
            "  inflating: Training/DepthImages/Depth_366.png  \n",
            "  inflating: Training/DepthImages/Depth_368.png  \n",
            "  inflating: Training/DepthImages/Depth_369.png  \n",
            "  inflating: Training/DepthImages/Depth_371.png  \n",
            "  inflating: Training/DepthImages/Depth_372.png  \n",
            "  inflating: Training/DepthImages/Depth_375.png  \n",
            "  inflating: Training/DepthImages/Depth_376.png  \n",
            "  inflating: Training/DepthImages/Depth_377.png  \n",
            "  inflating: Training/DepthImages/Depth_378.png  \n",
            "  inflating: Training/DepthImages/Depth_379.png  \n",
            "  inflating: Training/DepthImages/Depth_380.png  \n",
            "  inflating: Training/DepthImages/Depth_381.png  \n",
            "  inflating: Training/DepthImages/Depth_382.png  \n",
            "  inflating: Training/DepthImages/Depth_387.png  \n",
            "  inflating: Training/DepthImages/Depth_389.png  \n",
            "  inflating: Training/DepthImages/Depth_390.png  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1"
      ],
      "metadata": {
        "id": "w_VHXDVFSh0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "# ============\n",
        "# CONFIG\n",
        "# ============\n",
        "# Set this to the folder that contains: Training/, Test/\n",
        "# Example (if you uploaded a zip and extracted under /content/data):\n",
        "DATA_ROOT = Path(\"/content/data\")\n",
        "\n",
        "TRAIN_DIR = DATA_ROOT / \"Training\"\n",
        "TEST_DIR  = DATA_ROOT / \"Test\"\n",
        "\n",
        "TRAIN_CSV = TRAIN_DIR / \"Train.csv\"\n",
        "TEST_CSV  = TEST_DIR / \"Test.csv\"\n",
        "\n",
        "TRAIN_RGB_DIR   = TRAIN_DIR / \"RGBImages\"\n",
        "TRAIN_DEPTH_DIR = TRAIN_DIR / \"DepthImages\"\n",
        "\n",
        "TEST_RGB_DIR    = TEST_DIR / \"RGBImages\"\n",
        "TEST_DEPTH_DIR  = TEST_DIR / \"DepthImages\"\n",
        "\n",
        "print(\"DATA_ROOT:\", DATA_ROOT)\n",
        "print(\"Train CSV exists:\", TRAIN_CSV.exists(), TRAIN_CSV)\n",
        "print(\"Test  CSV exists:\", TEST_CSV.exists(), TEST_CSV)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMLUpjExP7HS",
        "outputId": "62149b1d-d948-4d75-d301-29a336770d85"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DATA_ROOT: /content/data\n",
            "Train CSV exists: True /content/data/Training/Train.csv\n",
            "Test  CSV exists: True /content/data/Test/Test.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(TRAIN_CSV)\n",
        "test_df  = pd.read_csv(TEST_CSV)\n",
        "\n",
        "print(\"Train shape:\", train_df.shape)\n",
        "print(\"Test  shape:\", test_df.shape)\n",
        "print(\"\\nTrain columns:\", list(train_df.columns))\n",
        "print(\"Test  columns:\", list(test_df.columns))\n",
        "\n",
        "# Basic column expectations\n",
        "required_train_cols = {\"image_id\",\"Height\",\"Diameter\",\"LeafArea\",\"FreshWeightShoot\",\"DryWeightShoot\"}\n",
        "missing_train_cols = required_train_cols - set(train_df.columns)\n",
        "print(\"\\nMissing required train columns:\", missing_train_cols)\n",
        "\n",
        "print(\"\\nTrain head:\\n\", train_df.head(3))\n",
        "print(\"\\nTest head:\\n\", test_df.head(3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4psoN0mmQjo0",
        "outputId": "71ae9f78-9a26-4eb4-e2f9-68025af0af65"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (231, 7)\n",
            "Test  shape: (76, 2)\n",
            "\n",
            "Train columns: ['image_id', 'Height', 'Diameter', 'LeafArea', 'FreshWeightShoot', 'Variety', 'DryWeightShoot']\n",
            "Test  columns: ['image_id', 'DryWeightShoot']\n",
            "\n",
            "Missing required train columns: set()\n",
            "\n",
            "Train head:\n",
            "    image_id  Height  Diameter  LeafArea  FreshWeightShoot   Variety  \\\n",
            "0        15     5.1      16.1      87.6               3.2  Aphylion   \n",
            "1        16     9.6      18.2     102.4               3.8  Aphylion   \n",
            "2        18     6.2      17.6      84.5               3.1  Aphylion   \n",
            "\n",
            "   DryWeightShoot  \n",
            "0            0.16  \n",
            "1            0.16  \n",
            "2            0.14  \n",
            "\n",
            "Test head:\n",
            "    image_id  DryWeightShoot\n",
            "0        17             NaN\n",
            "1        47             NaN\n",
            "2        53             NaN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def expected_paths(image_id: int, rgb_dir: Path, depth_dir: Path):\n",
        "    rgb_path = rgb_dir / f\"RGB_{image_id}.png\"\n",
        "    depth_path = depth_dir / f\"Depth_{image_id}.png\"\n",
        "    return rgb_path, depth_path\n",
        "\n",
        "def check_split(df: pd.DataFrame, rgb_dir: Path, depth_dir: Path, name: str):\n",
        "    missing_rgb = []\n",
        "    missing_depth = []\n",
        "\n",
        "    for image_id in df[\"image_id\"].astype(int).tolist():\n",
        "        rgb_path, depth_path = expected_paths(image_id, rgb_dir, depth_dir)\n",
        "        if not rgb_path.exists():\n",
        "            missing_rgb.append(str(rgb_path))\n",
        "        if not depth_path.exists():\n",
        "            missing_depth.append(str(depth_path))\n",
        "\n",
        "    print(f\"\\n[{name}] total rows:\", len(df))\n",
        "    print(f\"[{name}] missing RGB:\", len(missing_rgb))\n",
        "    print(f\"[{name}] missing Depth:\", len(missing_depth))\n",
        "\n",
        "    if missing_rgb:\n",
        "        print(\"  Example missing RGB:\", missing_rgb[0])\n",
        "    if missing_depth:\n",
        "        print(\"  Example missing Depth:\", missing_depth[0])\n",
        "\n",
        "    return missing_rgb, missing_depth\n",
        "\n",
        "train_missing_rgb, train_missing_depth = check_split(train_df, TRAIN_RGB_DIR, TRAIN_DEPTH_DIR, \"TRAIN\")\n",
        "test_missing_rgb,  test_missing_depth  = check_split(test_df,  TEST_RGB_DIR,  TEST_DEPTH_DIR,  \"TEST\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jmhLMJHQnm2",
        "outputId": "7e54b7d1-f0f4-4f42-8970-e514724190cc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[TRAIN] total rows: 231\n",
            "[TRAIN] missing RGB: 1\n",
            "[TRAIN] missing Depth: 0\n",
            "  Example missing RGB: /content/data/Training/RGBImages/RGB_332.png\n",
            "\n",
            "[TEST] total rows: 76\n",
            "[TEST] missing RGB: 0\n",
            "[TEST] missing Depth: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pick a sample id that exists in train\n",
        "sample_id = int(train_df[\"image_id\"].iloc[0])\n",
        "\n",
        "rgb_path, depth_path = expected_paths(sample_id, TRAIN_RGB_DIR, TRAIN_DEPTH_DIR)\n",
        "print(\"Sample image_id:\", sample_id)\n",
        "print(\"RGB path:\", rgb_path)\n",
        "print(\"Depth path:\", depth_path)\n",
        "\n",
        "rgb = Image.open(rgb_path)\n",
        "depth = Image.open(depth_path)\n",
        "\n",
        "print(\"\\nRGB mode/size:\", rgb.mode, rgb.size)\n",
        "print(\"Depth mode/size:\", depth.mode, depth.size)\n",
        "\n",
        "# Convert depth to numpy for quick stats\n",
        "depth_np = np.array(depth)\n",
        "print(\"\\nDepth dtype:\", depth_np.dtype)\n",
        "print(\"Depth min/max:\", int(depth_np.min()), int(depth_np.max()))\n",
        "print(\"Depth percentiles (50/90/95/99):\",\n",
        "      np.percentile(depth_np, [50,90,95,99]).astype(int))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvrjE_H6RGMT",
        "outputId": "da375dbf-dc63-4188-dacf-51cbeaa0cdbd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample image_id: 15\n",
            "RGB path: /content/data/Training/RGBImages/RGB_15.png\n",
            "Depth path: /content/data/Training/DepthImages/Depth_15.png\n",
            "\n",
            "RGB mode/size: RGB (1920, 1080)\n",
            "Depth mode/size: I;16 (1920, 1080)\n",
            "\n",
            "Depth dtype: uint16\n",
            "Depth min/max: 0 25729\n",
            "Depth percentiles (50/90/95/99): [1140 1324 1361 1374]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n===== SUMMARY =====\")\n",
        "print(\"Train rows:\", len(train_df), \" Test rows:\", len(test_df))\n",
        "print(\"Train missing RGB:\", len(train_missing_rgb), \" Train missing Depth:\", len(train_missing_depth))\n",
        "print(\"Test  missing RGB:\", len(test_missing_rgb),  \" Test  missing Depth:\", len(test_missing_depth))\n",
        "print(\"===================\\n\")\n",
        "\n",
        "if len(train_missing_rgb) == 0 and len(train_missing_depth) == 0 and len(test_missing_rgb) == 0 and len(test_missing_depth) == 0:\n",
        "    print(\" Step 1 passed: CSVs load and all images are present.\")\n",
        "else:\n",
        "    print(\" Step 1 found missing files. We'll fix paths / IDs before continuing.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAZnGQZSRXXp",
        "outputId": "73f477db-5fdc-4373-a5dd-02fbe5e78ea8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== SUMMARY =====\n",
            "Train rows: 231  Test rows: 76\n",
            "Train missing RGB: 1  Train missing Depth: 0\n",
            "Test  missing RGB: 0  Test  missing Depth: 0\n",
            "===================\n",
            "\n",
            " Step 1 found missing files. We'll fix paths / IDs before continuing.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "missing_id = 332\n",
        "\n",
        "# confirm it exists in Train.csv\n",
        "print(\"Rows in Train.csv with image_id=332:\")\n",
        "display(train_df[train_df[\"image_id\"] == missing_id])\n",
        "\n",
        "# check if any file contains \"332\" in the RGB directory\n",
        "matches = sorted([p.name for p in TRAIN_RGB_DIR.glob(\"*332*\")])\n",
        "print(\"\\nFiles in Training/RGBImages containing '332':\", matches)\n",
        "\n",
        "# also check in Test RGB dir just in case\n",
        "matches_test = sorted([p.name for p in TEST_RGB_DIR.glob(\"*332*\")])\n",
        "print(\"\\nFiles in Test/RGBImages containing '332':\", matches_test)\n",
        "\n",
        "# list a few nearby IDs (to see naming pattern)\n",
        "nearby = sorted([p.name for p in TRAIN_RGB_DIR.glob(\"RGB_33*.png\")])[:20]\n",
        "print(\"\\nSome nearby RGB_33*.png files:\", nearby)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "aLg65VJhRaD8",
        "outputId": "107b43a8-e554-4dab-8204-2bf6409d460a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows in Train.csv with image_id=332:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "     image_id  Height  Diameter  LeafArea  FreshWeightShoot   Variety  \\\n",
              "166       332    15.0      29.0    4153.3             232.5  Salanova   \n",
              "\n",
              "     DryWeightShoot  \n",
              "166           10.49  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3c0f7860-f155-445b-8f72-17b5b10b0715\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_id</th>\n",
              "      <th>Height</th>\n",
              "      <th>Diameter</th>\n",
              "      <th>LeafArea</th>\n",
              "      <th>FreshWeightShoot</th>\n",
              "      <th>Variety</th>\n",
              "      <th>DryWeightShoot</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>166</th>\n",
              "      <td>332</td>\n",
              "      <td>15.0</td>\n",
              "      <td>29.0</td>\n",
              "      <td>4153.3</td>\n",
              "      <td>232.5</td>\n",
              "      <td>Salanova</td>\n",
              "      <td>10.49</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3c0f7860-f155-445b-8f72-17b5b10b0715')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3c0f7860-f155-445b-8f72-17b5b10b0715 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3c0f7860-f155-445b-8f72-17b5b10b0715');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"print(\\\"\\\\nSome nearby RGB_33*\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"image_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 332,\n        \"max\": 332,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          332\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Height\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 15.0,\n        \"max\": 15.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          15.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Diameter\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 29.0,\n        \"max\": 29.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          29.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"LeafArea\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 4153.3,\n        \"max\": 4153.3,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          4153.3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"FreshWeightShoot\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 232.5,\n        \"max\": 232.5,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          232.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Variety\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Salanova\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"DryWeightShoot\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 10.49,\n        \"max\": 10.49,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          10.49\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Files in Training/RGBImages containing '332': []\n",
            "\n",
            "Files in Test/RGBImages containing '332': []\n",
            "\n",
            "Some nearby RGB_33*.png files: ['RGB_330.png', 'RGB_334.png', 'RGB_335.png', 'RGB_337.png', 'RGB_338.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows whose RGB or Depth file is missing (train only)\n",
        "def has_both_images(image_id: int, rgb_dir: Path, depth_dir: Path) -> bool:\n",
        "    rgb_path = rgb_dir / f\"RGB_{image_id}.png\"\n",
        "    depth_path = depth_dir / f\"Depth_{image_id}.png\"\n",
        "    return rgb_path.exists() and depth_path.exists()\n",
        "\n",
        "mask = train_df[\"image_id\"].astype(int).apply(lambda x: has_both_images(x, TRAIN_RGB_DIR, TRAIN_DEPTH_DIR))\n",
        "train_df_clean = train_df[mask].reset_index(drop=True)\n",
        "\n",
        "print(\"Original train rows:\", len(train_df))\n",
        "print(\"Cleaned  train rows:\", len(train_df_clean))\n",
        "print(\"Dropped IDs:\", sorted(set(train_df[\"image_id\"]) - set(train_df_clean[\"image_id\"])))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jZhZft3SDJu",
        "outputId": "adea2bef-c46d-4c9f-b1f3-4b60e0307317"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original train rows: 231\n",
            "Cleaned  train rows: 230\n",
            "Dropped IDs: [332]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Are there any duplicate IDs?\n",
        "print(\"Test duplicate image_ids:\", test_df[\"image_id\"].duplicated().sum())\n",
        "\n",
        "# Do all Test.csv IDs have corresponding files?\n",
        "test_missing_rgb2, test_missing_depth2 = check_split(test_df, TEST_RGB_DIR, TEST_DEPTH_DIR, \"TEST (recheck)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAk6vFieSGmg",
        "outputId": "7cb4bbcf-ad13-42f0-90a7-dc7bc7221874"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test duplicate image_ids: 0\n",
            "\n",
            "[TEST (recheck)] total rows: 76\n",
            "[TEST (recheck)] missing RGB: 0\n",
            "[TEST (recheck)] missing Depth: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2"
      ],
      "metadata": {
        "id": "bTLw3MqLSlkK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Use cleaned train df\n",
        "train_df = train_df_clean\n",
        "\n",
        "def load_depth_uint16(path: Path) -> np.ndarray:\n",
        "    d = Image.open(path)\n",
        "    return np.array(d)\n",
        "\n",
        "# Compute a robust scale based on p99 across images\n",
        "per_image_p99 = []\n",
        "for image_id in tqdm(train_df[\"image_id\"].astype(int).tolist(), desc=\"Computing per-image depth p99\"):\n",
        "    depth_path = TRAIN_DEPTH_DIR / f\"Depth_{image_id}.png\"\n",
        "    d = load_depth_uint16(depth_path)\n",
        "    per_image_p99.append(np.percentile(d, 99))\n",
        "\n",
        "DEPTH_P99 = float(np.percentile(per_image_p99, 50))  # median of per-image p99 values\n",
        "print(\"Median(per-image depth p99):\", DEPTH_P99)\n",
        "\n",
        "# Safety fallback\n",
        "if DEPTH_P99 <= 0:\n",
        "    DEPTH_P99 = 65535.0\n",
        "\n",
        "print(\"DEPTH_P99 used for normalization:\", DEPTH_P99)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5VlvCchSJC4",
        "outputId": "ac528f77-4176-4dc1-a00d-4df0a400e3cd"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Computing per-image depth p99: 100%|| 230/230 [00:07<00:00, 31.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Median(per-image depth p99): 1374.0\n",
            "DEPTH_P99 used for normalization: 1374.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "IMG_SIZE = 256\n",
        "\n",
        "# RGB normalization (simple, safe baseline)\n",
        "RGB_MEAN = (0.5, 0.5, 0.5)\n",
        "RGB_STD  = (0.25, 0.25, 0.25)\n",
        "\n",
        "def load_rgb_tensor(path: Path, size=IMG_SIZE) -> torch.Tensor:\n",
        "    img = Image.open(path).convert(\"RGB\").resize((size, size), Image.BILINEAR)\n",
        "    arr = np.asarray(img).astype(np.float32) / 255.0  # HWC, [0,1]\n",
        "    # normalize\n",
        "    arr = (arr - np.array(RGB_MEAN)) / np.array(RGB_STD)\n",
        "    # HWC -> CHW\n",
        "    arr = np.transpose(arr, (2,0,1))\n",
        "    return torch.from_numpy(arr).float()\n",
        "\n",
        "def load_depth_tensor(path: Path, size=IMG_SIZE, depth_p99=DEPTH_P99) -> torch.Tensor:\n",
        "    img = Image.open(path)  # 16-bit\n",
        "    img = img.resize((size, size), Image.NEAREST)  # nearest preserves discrete depth better\n",
        "    d = np.asarray(img).astype(np.float32)\n",
        "    # robust clip + normalize to [0,1]\n",
        "    d = np.clip(d, 0, depth_p99) / depth_p99\n",
        "    # add channel dim: 1xHxW\n",
        "    d = d[None, :, :]\n",
        "    return torch.from_numpy(d).float()\n",
        "\n",
        "# Auxiliary standardization stats (computed from train)\n",
        "AUX_COLS = [\"Height\",\"Diameter\",\"LeafArea\",\"FreshWeightShoot\"]\n",
        "aux_mean = train_df[AUX_COLS].mean().values.astype(np.float32)\n",
        "aux_std  = train_df[AUX_COLS].std(ddof=0).values.astype(np.float32)\n",
        "aux_std[aux_std == 0] = 1.0\n",
        "\n",
        "print(\"Aux mean:\", aux_mean)\n",
        "print(\"Aux std :\", aux_std)\n",
        "\n",
        "class LettuceDataset(Dataset):\n",
        "    def __init__(self, df: pd.DataFrame, rgb_dir: Path, depth_dir: Path, train_mode: bool):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.rgb_dir = rgb_dir\n",
        "        self.depth_dir = depth_dir\n",
        "        self.train_mode = train_mode\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        row = self.df.iloc[idx]\n",
        "        image_id = int(row[\"image_id\"])\n",
        "\n",
        "        rgb_path   = self.rgb_dir / f\"RGB_{image_id}.png\"\n",
        "        depth_path = self.depth_dir / f\"Depth_{image_id}.png\"\n",
        "\n",
        "        rgb = load_rgb_tensor(rgb_path)\n",
        "        depth = load_depth_tensor(depth_path)\n",
        "\n",
        "        x = torch.cat([rgb, depth], dim=0)  # 4xHxW\n",
        "\n",
        "        if self.train_mode:\n",
        "            y_main = torch.tensor([float(row[\"DryWeightShoot\"])], dtype=torch.float32)\n",
        "\n",
        "            aux = row[AUX_COLS].values.astype(np.float32)\n",
        "            aux = (aux - aux_mean) / aux_std\n",
        "            y_aux = torch.from_numpy(aux)\n",
        "\n",
        "            return x, y_main, y_aux, image_id\n",
        "        else:\n",
        "            return x, image_id\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yDarYQXSrJP",
        "outputId": "1797e675-f9d7-4e66-aa2f-6cf262f14b2e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aux mean: [  12.399131   22.873478 1894.5353    117.21435 ]\n",
            "Aux std : [   4.822682    6.386966 1577.2528    110.85202 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds = LettuceDataset(train_df.iloc[:5], TRAIN_RGB_DIR, TRAIN_DEPTH_DIR, train_mode=True)\n",
        "\n",
        "x, y_main, y_aux, image_id = ds[0]\n",
        "print(\"image_id:\", image_id)\n",
        "print(\"x shape:\", x.shape)           # expect [4, 256, 256]\n",
        "print(\"y_main:\", y_main, y_main.shape)  # expect [1]\n",
        "print(\"y_aux shape:\", y_aux.shape)   # expect [4]\n",
        "\n",
        "# Check value ranges\n",
        "print(\"RGB channels stats (mean/std):\", x[:3].mean().item(), x[:3].std().item())\n",
        "print(\"Depth channel stats (min/max):\", x[3].min().item(), x[3].max().item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2F7Ys_WS6qJ",
        "outputId": "c4a3a31d-2c19-4372-eb36-4dc7c7f62ab2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "image_id: 15\n",
            "x shape: torch.Size([4, 256, 256])\n",
            "y_main: tensor([0.1600]) torch.Size([1])\n",
            "y_aux shape: torch.Size([4])\n",
            "RGB channels stats (mean/std): -0.180261590139177 0.5769466877044992\n",
            "Depth channel stats (min/max): 0.0 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "loader = DataLoader(ds, batch_size=2, shuffle=False, num_workers=0)\n",
        "\n",
        "batch = next(iter(loader))\n",
        "xb, yb, yauxb, ids = batch\n",
        "print(\"Batch x:\", xb.shape)      # [2,4,256,256]\n",
        "print(\"Batch y:\", yb.shape)      # [2,1]\n",
        "print(\"Batch yaux:\", yauxb.shape)# [2,4]\n",
        "print(\"Batch ids:\", ids)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1w-zPKoAS8rJ",
        "outputId": "6dc90eac-5c31-426b-a582-abe261f014c5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch x: torch.Size([2, 4, 256, 256])\n",
            "Batch y: torch.Size([2, 1])\n",
            "Batch yaux: torch.Size([2, 4])\n",
            "Batch ids: tensor([15, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3"
      ],
      "metadata": {
        "id": "CfPyAYssTPIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SmallCNNMultiTask(nn.Module):\n",
        "    def __init__(self, dropout=0.3):\n",
        "        super().__init__()\n",
        "\n",
        "        def block(in_ch, out_ch):\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1, bias=False),\n",
        "                nn.BatchNorm2d(out_ch),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool2d(2)\n",
        "            )\n",
        "\n",
        "        self.backbone = nn.Sequential(\n",
        "            block(4, 32),    # 256 -> 128\n",
        "            block(32, 64),   # 128 -> 64\n",
        "            block(64, 128),  # 64 -> 32\n",
        "            block(128, 256)  # 32 -> 16\n",
        "        )\n",
        "\n",
        "        self.pool = nn.AdaptiveAvgPool2d(1)  # -> [B,256,1,1]\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "        self.head_main = nn.Linear(128, 1)   # DryWeightShoot\n",
        "        self.head_aux  = nn.Linear(128, 4)   # Height, Diameter, LeafArea, FreshWeightShoot (standardized)\n",
        "\n",
        "    def forward(self, x):\n",
        "        feats = self.backbone(x)\n",
        "        feats = self.pool(feats).flatten(1)      # [B,256]\n",
        "        h = self.fc(feats)                       # [B,128]\n",
        "        y_main = self.head_main(h)               # [B,1]\n",
        "        y_aux  = self.head_aux(h)                # [B,4]\n",
        "        return y_main, y_aux\n"
      ],
      "metadata": {
        "id": "wUc7qLwTS_jN"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "model = SmallCNNMultiTask(dropout=0.3).to(device)\n",
        "\n",
        "ds_small = LettuceDataset(train_df.iloc[:8], TRAIN_RGB_DIR, TRAIN_DEPTH_DIR, train_mode=True)\n",
        "loader_small = DataLoader(ds_small, batch_size=4, shuffle=False, num_workers=0)\n",
        "\n",
        "xb, yb, yauxb, ids = next(iter(loader_small))\n",
        "\n",
        "# Force float32 + move to GPU\n",
        "xb = xb.to(device).float()\n",
        "\n",
        "with torch.no_grad():\n",
        "    pred_main, pred_aux = model(xb)\n",
        "\n",
        "print(\"xb dtype:\", xb.dtype)\n",
        "print(\"pred_main shape:\", pred_main.shape)  # [4,1]\n",
        "print(\"pred_aux shape :\", pred_aux.shape)   # [4,4]\n",
        "print(\"pred_main sample:\", pred_main[:2].view(-1).detach().cpu().numpy())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBTiwIsVTS9_",
        "outputId": "d3ce4a6d-2ae7-48dd-e6d6-59dba7a31886"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "xb dtype: torch.float32\n",
            "pred_main shape: torch.Size([4, 1])\n",
            "pred_aux shape : torch.Size([4, 4])\n",
            "pred_main sample: [0.5440216  0.31083065]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mae = nn.L1Loss()\n",
        "mse = nn.MSELoss()\n",
        "\n",
        "LAMBDA_AUX = 0.3  # starting point\n",
        "\n",
        "def compute_loss(pred_main, pred_aux, y_main, y_aux, lam=LAMBDA_AUX):\n",
        "    loss_main = mae(pred_main, y_main)\n",
        "    loss_aux  = mse(pred_aux, y_aux)\n",
        "    loss_total = loss_main + lam * loss_aux\n",
        "    return loss_total, loss_main, loss_aux\n"
      ],
      "metadata": {
        "id": "xfqVbm_hWU3M"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4"
      ],
      "metadata": {
        "id": "JW5BTKomXO8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "import time\n",
        "\n",
        "# Use a very small subset\n",
        "tiny_df = train_df.sample(n=10, random_state=42).reset_index(drop=True)\n",
        "\n",
        "tiny_ds = LettuceDataset(tiny_df, TRAIN_RGB_DIR, TRAIN_DEPTH_DIR, train_mode=True)\n",
        "tiny_loader = DataLoader(tiny_ds, batch_size=5, shuffle=True, num_workers=0)\n",
        "\n",
        "print(\"Tiny dataset size:\", len(tiny_ds))\n",
        "print(tiny_df[[\"image_id\",\"DryWeightShoot\"]].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDLT3nAwW64b",
        "outputId": "056bb4ec-7a65-46c8-ec03-a5a6e1966808"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tiny dataset size: 10\n",
            "   image_id  DryWeightShoot\n",
            "0       371           10.11\n",
            "1        70            1.82\n",
            "2       101            1.79\n",
            "3       183            2.80\n",
            "4       110            1.42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SmallCNNMultiTask(dropout=0.0).to(device)  # dropout off to help memorization\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.0)\n",
        "\n",
        "mae = nn.L1Loss()\n",
        "mse = nn.MSELoss()\n",
        "LAMBDA_AUX = 0.3\n",
        "\n",
        "def compute_loss(pred_main, pred_aux, y_main, y_aux, lam=LAMBDA_AUX):\n",
        "    loss_main = mae(pred_main, y_main)\n",
        "    loss_aux  = mse(pred_aux, y_aux)\n",
        "    return loss_main + lam * loss_aux, loss_main, loss_aux\n",
        "\n",
        "model.train()\n",
        "num_epochs = 200\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    total, main_total, aux_total = 0.0, 0.0, 0.0\n",
        "\n",
        "    for xb, yb, yauxb, ids in tiny_loader:\n",
        "        xb = xb.to(device).float()\n",
        "        yb = yb.to(device).float()\n",
        "        yauxb = yauxb.to(device).float()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        pred_main, pred_aux = model(xb)\n",
        "\n",
        "        loss, loss_main, loss_aux = compute_loss(pred_main, pred_aux, yb, yauxb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total += loss.item() * xb.size(0)\n",
        "        main_total += loss_main.item() * xb.size(0)\n",
        "        aux_total += loss_aux.item() * xb.size(0)\n",
        "\n",
        "    total /= len(tiny_ds)\n",
        "    main_total /= len(tiny_ds)\n",
        "    aux_total /= len(tiny_ds)\n",
        "\n",
        "    if epoch % 20 == 0 or epoch == 1:\n",
        "        print(f\"Epoch {epoch:03d} | total={total:.4f} | main(MAE)={main_total:.4f} | aux(MSE)={aux_total:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cu0RPhBfXQ4L",
        "outputId": "fdd9ef94-3b8c-4c1a-c49f-a9d3a23b9300"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | total=3.6423 | main(MAE)=3.4596 | aux(MSE)=0.6090\n",
            "Epoch 020 | total=0.6121 | main(MAE)=0.5268 | aux(MSE)=0.2844\n",
            "Epoch 040 | total=0.3291 | main(MAE)=0.2918 | aux(MSE)=0.1243\n",
            "Epoch 060 | total=0.6432 | main(MAE)=0.6089 | aux(MSE)=0.1146\n",
            "Epoch 080 | total=0.6976 | main(MAE)=0.6700 | aux(MSE)=0.0919\n",
            "Epoch 100 | total=0.2190 | main(MAE)=0.1967 | aux(MSE)=0.0745\n",
            "Epoch 120 | total=0.4997 | main(MAE)=0.4697 | aux(MSE)=0.1000\n",
            "Epoch 140 | total=0.6819 | main(MAE)=0.6505 | aux(MSE)=0.1049\n",
            "Epoch 160 | total=0.1610 | main(MAE)=0.1448 | aux(MSE)=0.0541\n",
            "Epoch 180 | total=0.3104 | main(MAE)=0.2861 | aux(MSE)=0.0810\n",
            "Epoch 200 | total=0.3165 | main(MAE)=0.2971 | aux(MSE)=0.0647\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "rows = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in range(len(tiny_ds)):\n",
        "        x, y, yaux, image_id = tiny_ds[i]\n",
        "        x = x.unsqueeze(0).to(device).float()\n",
        "        pred_main, _ = model(x)\n",
        "        pred = float(pred_main.item())\n",
        "        rows.append((image_id, float(y.item()), pred, abs(pred - float(y.item()))))\n",
        "\n",
        "res = pd.DataFrame(rows, columns=[\"image_id\",\"true\",\"pred\",\"abs_error\"]).sort_values(\"abs_error\")\n",
        "print(res)\n",
        "print(\"\\nMean abs error on tiny set:\", res[\"abs_error\"].mean())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LE48swwAXSXh",
        "outputId": "ac35a301-4188-458b-e0b3-2fa42010cc30"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   image_id   true       pred  abs_error\n",
            "1        70   1.82   1.809090   0.010910\n",
            "2       101   1.79   1.825552   0.035552\n",
            "3       183   2.80   2.875254   0.075254\n",
            "7       187   2.24   2.421997   0.181997\n",
            "4       110   1.42   1.606386   0.186386\n",
            "9        93   1.24   1.469445   0.229445\n",
            "5       193   3.63   3.866493   0.236493\n",
            "6       176   4.10   4.439178   0.339178\n",
            "8       258   7.63   8.568734   0.938734\n",
            "0       371  10.11  12.031196   1.921196\n",
            "\n",
            "Mean abs error on tiny set: 0.41551449298858645\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4.1"
      ],
      "metadata": {
        "id": "bexfQvsaYjaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SmallCNNMultiTask(dropout=0.0).to(device)\n",
        "\n",
        "# --- Freeze BatchNorm stats (keeps affine params trainable) ---\n",
        "for m in model.modules():\n",
        "    if isinstance(m, nn.BatchNorm2d):\n",
        "        m.eval()  # freezes running mean/var\n",
        "        m.requires_grad_(True)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=3e-4, weight_decay=0.0)\n",
        "\n",
        "mae = nn.L1Loss()\n",
        "mse = nn.MSELoss()\n",
        "LAMBDA_AUX = 0.2  # slightly lower to focus on main target\n",
        "\n",
        "def compute_loss(pred_main, pred_aux, y_main, y_aux, lam=LAMBDA_AUX):\n",
        "    loss_main = mae(pred_main, y_main)\n",
        "    loss_aux  = mse(pred_aux, y_aux)\n",
        "    return loss_main + lam * loss_aux, loss_main, loss_aux\n",
        "\n",
        "model.train()\n",
        "num_epochs = 400\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    total, main_total, aux_total = 0.0, 0.0, 0.0\n",
        "\n",
        "    for xb, yb, yauxb, ids in tiny_loader:\n",
        "        xb = xb.to(device).float()\n",
        "        yb = yb.to(device).float()\n",
        "        yauxb = yauxb.to(device).float()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        pred_main, pred_aux = model(xb)\n",
        "        loss, loss_main, loss_aux = compute_loss(pred_main, pred_aux, yb, yauxb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total += loss.item() * xb.size(0)\n",
        "        main_total += loss_main.item() * xb.size(0)\n",
        "        aux_total += loss_aux.item() * xb.size(0)\n",
        "\n",
        "    total /= len(tiny_ds)\n",
        "    main_total /= len(tiny_ds)\n",
        "    aux_total /= len(tiny_ds)\n",
        "\n",
        "    if epoch % 50 == 0 or epoch == 1:\n",
        "        print(f\"Epoch {epoch:03d} | total={total:.4f} | main(MAE)={main_total:.4f} | aux(MSE)={aux_total:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6yTJKUoYE4n",
        "outputId": "fdf72d6f-0e3b-4b59-a2eb-3943e807f017"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | total=3.7578 | main(MAE)=3.6613 | aux(MSE)=0.4827\n",
            "Epoch 050 | total=0.6550 | main(MAE)=0.5858 | aux(MSE)=0.3460\n",
            "Epoch 100 | total=0.3541 | main(MAE)=0.3356 | aux(MSE)=0.0925\n",
            "Epoch 150 | total=0.5038 | main(MAE)=0.4839 | aux(MSE)=0.0996\n",
            "Epoch 200 | total=0.3576 | main(MAE)=0.3410 | aux(MSE)=0.0830\n",
            "Epoch 250 | total=0.3382 | main(MAE)=0.3181 | aux(MSE)=0.1008\n",
            "Epoch 300 | total=0.3012 | main(MAE)=0.2871 | aux(MSE)=0.0707\n",
            "Epoch 350 | total=0.1876 | main(MAE)=0.1762 | aux(MSE)=0.0571\n",
            "Epoch 400 | total=0.2498 | main(MAE)=0.2371 | aux(MSE)=0.0635\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "rows = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in range(len(tiny_ds)):\n",
        "        x, y, yaux, image_id = tiny_ds[i]\n",
        "        x = x.unsqueeze(0).to(device).float()\n",
        "        pred_main, _ = model(x)\n",
        "        pred = float(pred_main.item())\n",
        "        rows.append((image_id, float(y.item()), pred, abs(pred - float(y.item()))))\n",
        "\n",
        "res = pd.DataFrame(rows, columns=[\"image_id\",\"true\",\"pred\",\"abs_error\"]).sort_values(\"abs_error\")\n",
        "print(res)\n",
        "print(\"\\nMean abs error on tiny set:\", res[\"abs_error\"].mean())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKS8zEKLYlWS",
        "outputId": "7da3bea5-0073-4565-c578-f2363d9378a3"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   image_id   true       pred  abs_error\n",
            "8       258   7.63   7.580428   0.049572\n",
            "2       101   1.79   1.705294   0.084706\n",
            "1        70   1.82   1.725532   0.094468\n",
            "5       193   3.63   3.519864   0.110137\n",
            "9        93   1.24   1.105059   0.134941\n",
            "4       110   1.42   1.252824   0.167176\n",
            "3       183   2.80   2.632094   0.167906\n",
            "6       176   4.10   3.924866   0.175134\n",
            "0       371  10.11  10.335894   0.225894\n",
            "7       187   2.24   1.991558   0.248442\n",
            "\n",
            "Mean abs error on tiny set: 0.14583749771118165\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5"
      ],
      "metadata": {
        "id": "Zi4DKXFNaUiG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Use cleaned training data\n",
        "df = train_df.copy()\n",
        "\n",
        "train_split, val_split = train_test_split(\n",
        "    df,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "train_split = train_split.reset_index(drop=True)\n",
        "val_split   = val_split.reset_index(drop=True)\n",
        "\n",
        "print(\"Train split:\", train_split.shape)\n",
        "print(\"Val split  :\", val_split.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jdj8rRcaIV2",
        "outputId": "3b4af12c-4f7b-450b-ac06-9f22336f6e4d"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train split: (184, 7)\n",
            "Val split  : (46, 7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 8\n",
        "\n",
        "train_ds = LettuceDataset(train_split, TRAIN_RGB_DIR, TRAIN_DEPTH_DIR, train_mode=True)\n",
        "val_ds   = LettuceDataset(val_split,   TRAIN_RGB_DIR, TRAIN_DEPTH_DIR, train_mode=True)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=0)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "\n",
        "print(\"Train batches:\", len(train_loader))\n",
        "print(\"Val batches  :\", len(val_loader))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcA_JS9YaXcW",
        "outputId": "77a931cf-d8f6-4bbb-98b8-cfb02f046749"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train batches: 23\n",
            "Val batches  : 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = SmallCNNMultiTask(dropout=0.3).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode=\"min\", factor=0.5, patience=8\n",
        ")\n",
        "\n",
        "mae = nn.L1Loss()\n",
        "mse = nn.MSELoss()\n",
        "LAMBDA_AUX = 0.3\n",
        "\n",
        "def train_one_epoch(model, loader):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_main = 0.0\n",
        "    total_aux  = 0.0\n",
        "    n = 0\n",
        "\n",
        "    for xb, yb, yauxb, ids in loader:\n",
        "        xb = xb.to(device).float()\n",
        "        yb = yb.to(device).float()\n",
        "        yauxb = yauxb.to(device).float()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        pred_main, pred_aux = model(xb)\n",
        "\n",
        "        loss_main = mae(pred_main, yb)\n",
        "        loss_aux  = mse(pred_aux, yauxb)\n",
        "        loss = loss_main + LAMBDA_AUX * loss_aux\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        bs = xb.size(0)\n",
        "        total_loss += loss.item() * bs\n",
        "        total_main += loss_main.item() * bs\n",
        "        total_aux  += loss_aux.item() * bs\n",
        "        n += bs\n",
        "\n",
        "    return total_loss / n, total_main / n, total_aux / n\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    total_main = 0.0\n",
        "    n = 0\n",
        "    for xb, yb, yauxb, ids in loader:\n",
        "        xb = xb.to(device).float()\n",
        "        yb = yb.to(device).float()\n",
        "        pred_main, pred_aux = model(xb)\n",
        "        loss_main = mae(pred_main, yb)\n",
        "        bs = xb.size(0)\n",
        "        total_main += loss_main.item() * bs\n",
        "        n += bs\n",
        "    return total_main / n  # MAE in grams\n",
        "\n",
        "# Early stopping\n",
        "best_val = float(\"inf\")\n",
        "best_state = None\n",
        "patience = 20\n",
        "patience_ctr = 0\n",
        "\n",
        "MAX_EPOCHS = 150\n",
        "\n",
        "for epoch in range(1, MAX_EPOCHS + 1):\n",
        "    tr_loss, tr_main, tr_aux = train_one_epoch(model, train_loader)\n",
        "    val_mae = evaluate(model, val_loader)\n",
        "\n",
        "    scheduler.step(val_mae)\n",
        "\n",
        "    lr = optimizer.param_groups[0][\"lr\"]\n",
        "\n",
        "    if val_mae < best_val - 1e-4:\n",
        "        best_val = val_mae\n",
        "        best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
        "        patience_ctr = 0\n",
        "    else:\n",
        "        patience_ctr += 1\n",
        "\n",
        "    if epoch % 10 == 0 or epoch == 1:\n",
        "        print(f\"Epoch {epoch:03d} | lr={lr:.2e} | train_main(MAE)={tr_main:.4f} | val_main(MAE)={val_mae:.4f}\")\n",
        "\n",
        "    if patience_ctr >= patience:\n",
        "        print(f\"Early stopping at epoch {epoch}. Best val MAE: {best_val:.4f}\")\n",
        "        break\n",
        "\n",
        "# Restore best\n",
        "model.load_state_dict(best_state)\n",
        "print(\"Restored best model. Best val MAE:\", best_val)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hB7O96X4aZH_",
        "outputId": "39ab1e8c-b1c6-4619-927a-83c903145845"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | lr=1.00e-03 | train_main(MAE)=3.3632 | val_main(MAE)=1.4550\n",
            "Epoch 010 | lr=1.00e-03 | train_main(MAE)=1.4062 | val_main(MAE)=1.0384\n",
            "Epoch 020 | lr=1.00e-03 | train_main(MAE)=1.1366 | val_main(MAE)=1.1815\n",
            "Epoch 030 | lr=5.00e-04 | train_main(MAE)=1.3936 | val_main(MAE)=0.8901\n",
            "Epoch 040 | lr=5.00e-04 | train_main(MAE)=1.0149 | val_main(MAE)=1.0835\n",
            "Epoch 050 | lr=5.00e-04 | train_main(MAE)=0.9421 | val_main(MAE)=0.7442\n",
            "Epoch 060 | lr=5.00e-04 | train_main(MAE)=1.1490 | val_main(MAE)=0.6693\n",
            "Epoch 070 | lr=2.50e-04 | train_main(MAE)=0.8683 | val_main(MAE)=0.5615\n",
            "Epoch 080 | lr=1.25e-04 | train_main(MAE)=0.8615 | val_main(MAE)=0.6208\n",
            "Epoch 090 | lr=1.25e-04 | train_main(MAE)=0.6970 | val_main(MAE)=0.5701\n",
            "Epoch 100 | lr=6.25e-05 | train_main(MAE)=0.7155 | val_main(MAE)=0.5747\n",
            "Early stopping at epoch 104. Best val MAE: 0.5598\n",
            "Restored best model. Best val MAE: 0.5598193044247834\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6"
      ],
      "metadata": {
        "id": "Dk6Y6urUjADX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 6A"
      ],
      "metadata": {
        "id": "ZzA6jbL4jBw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from PIL import ImageEnhance\n",
        "\n",
        "def apply_train_augmentations(rgb_img: Image.Image, depth_img: Image.Image):\n",
        "    # --- Random horizontal flip (same for both) ---\n",
        "    if random.random() < 0.5:\n",
        "        rgb_img = rgb_img.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "        depth_img = depth_img.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "\n",
        "    # --- Small rotation (same for both) ---\n",
        "    angle = random.uniform(-10, 10)\n",
        "    rgb_img = rgb_img.rotate(angle, resample=Image.BILINEAR)\n",
        "    depth_img = depth_img.rotate(angle, resample=Image.NEAREST)\n",
        "\n",
        "    # --- Mild color jitter (RGB only) ---\n",
        "    # brightness +/- 10%\n",
        "    if random.random() < 0.8:\n",
        "        rgb_img = ImageEnhance.Brightness(rgb_img).enhance(random.uniform(0.9, 1.1))\n",
        "        rgb_img = ImageEnhance.Contrast(rgb_img).enhance(random.uniform(0.9, 1.1))\n",
        "\n",
        "    return rgb_img, depth_img\n"
      ],
      "metadata": {
        "id": "5byRbte4abqo"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LettuceDataset(Dataset):\n",
        "    def __init__(self, df: pd.DataFrame, rgb_dir: Path, depth_dir: Path, train_mode: bool, augment: bool = False):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.rgb_dir = rgb_dir\n",
        "        self.depth_dir = depth_dir\n",
        "        self.train_mode = train_mode\n",
        "        self.augment = augment\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        row = self.df.iloc[idx]\n",
        "        image_id = int(row[\"image_id\"])\n",
        "\n",
        "        rgb_path   = self.rgb_dir / f\"RGB_{image_id}.png\"\n",
        "        depth_path = self.depth_dir / f\"Depth_{image_id}.png\"\n",
        "\n",
        "        # Load original PIL images\n",
        "        rgb_img = Image.open(rgb_path).convert(\"RGB\")\n",
        "        depth_img = Image.open(depth_path)  # 16-bit\n",
        "\n",
        "        # Apply aug only for training set\n",
        "        if self.train_mode and self.augment:\n",
        "            rgb_img, depth_img = apply_train_augmentations(rgb_img, depth_img)\n",
        "\n",
        "        # Resize\n",
        "        rgb_img = rgb_img.resize((IMG_SIZE, IMG_SIZE), Image.BILINEAR)\n",
        "        depth_img = depth_img.resize((IMG_SIZE, IMG_SIZE), Image.NEAREST)\n",
        "\n",
        "        # RGB -> tensor normalized\n",
        "        rgb_arr = np.asarray(rgb_img).astype(np.float32) / 255.0\n",
        "        rgb_arr = (rgb_arr - np.array(RGB_MEAN)) / np.array(RGB_STD)\n",
        "        rgb_arr = np.transpose(rgb_arr, (2,0,1))\n",
        "        rgb = torch.from_numpy(rgb_arr).float()\n",
        "\n",
        "        # Depth -> tensor normalized robustly\n",
        "        d = np.asarray(depth_img).astype(np.float32)\n",
        "        d = np.clip(d, 0, DEPTH_P99) / DEPTH_P99\n",
        "        depth = torch.from_numpy(d[None, :, :]).float()\n",
        "\n",
        "        x = torch.cat([rgb, depth], dim=0)  # 4xHxW\n",
        "\n",
        "        if self.train_mode:\n",
        "            y_main = torch.tensor([float(row[\"DryWeightShoot\"])], dtype=torch.float32)\n",
        "\n",
        "            aux = row[AUX_COLS].values.astype(np.float32)\n",
        "            aux = (aux - aux_mean) / aux_std\n",
        "            y_aux = torch.from_numpy(aux).float()\n",
        "\n",
        "            return x, y_main, y_aux, image_id\n",
        "        else:\n",
        "            return x, image_id\n"
      ],
      "metadata": {
        "id": "fVNkqLS2jGWd"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = LettuceDataset(train_split, TRAIN_RGB_DIR, TRAIN_DEPTH_DIR, train_mode=True, augment=True)\n",
        "val_ds   = LettuceDataset(val_split,   TRAIN_RGB_DIR, TRAIN_DEPTH_DIR, train_mode=True, augment=False)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=0)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SmallCNNMultiTask(dropout=0.3).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode=\"min\", factor=0.5, patience=8\n",
        ")\n",
        "\n",
        "best_val = float(\"inf\")\n",
        "best_state = None\n",
        "patience = 20\n",
        "patience_ctr = 0\n",
        "MAX_EPOCHS = 150\n",
        "\n",
        "for epoch in range(1, MAX_EPOCHS + 1):\n",
        "    tr_loss, tr_main, tr_aux = train_one_epoch(model, train_loader)\n",
        "    val_mae = evaluate(model, val_loader)\n",
        "\n",
        "    scheduler.step(val_mae)\n",
        "    lr = optimizer.param_groups[0][\"lr\"]\n",
        "\n",
        "    if val_mae < best_val - 1e-4:\n",
        "        best_val = val_mae\n",
        "        best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
        "        patience_ctr = 0\n",
        "    else:\n",
        "        patience_ctr += 1\n",
        "\n",
        "    if epoch % 10 == 0 or epoch == 1:\n",
        "        print(f\"Epoch {epoch:03d} | lr={lr:.2e} | train_main(MAE)={tr_main:.4f} | val_main(MAE)={val_mae:.4f}\")\n",
        "\n",
        "    if patience_ctr >= patience:\n",
        "        print(f\"Early stopping at epoch {epoch}. Best val MAE: {best_val:.4f}\")\n",
        "        break\n",
        "\n",
        "model.load_state_dict(best_state)\n",
        "print(\"Restored best model. Best val MAE:\", best_val)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0ywhR-BjHyZ",
        "outputId": "91ac9651-a2cb-475c-b11e-c0819cebf8bc"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | lr=1.00e-03 | train_main(MAE)=3.7098 | val_main(MAE)=2.1517\n",
            "Epoch 010 | lr=1.00e-03 | train_main(MAE)=1.7217 | val_main(MAE)=1.1764\n",
            "Epoch 020 | lr=1.00e-03 | train_main(MAE)=1.3256 | val_main(MAE)=0.7192\n",
            "Epoch 030 | lr=5.00e-04 | train_main(MAE)=1.2956 | val_main(MAE)=0.6194\n",
            "Epoch 040 | lr=2.50e-04 | train_main(MAE)=1.0981 | val_main(MAE)=1.3473\n",
            "Epoch 050 | lr=2.50e-04 | train_main(MAE)=1.0344 | val_main(MAE)=0.6424\n",
            "Epoch 060 | lr=2.50e-04 | train_main(MAE)=1.2556 | val_main(MAE)=0.7034\n",
            "Epoch 070 | lr=6.25e-05 | train_main(MAE)=1.0208 | val_main(MAE)=0.6256\n",
            "Early stopping at epoch 72. Best val MAE: 0.5053\n",
            "Restored best model. Best val MAE: 0.505272898984992\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 6 B"
      ],
      "metadata": {
        "id": "pYDmD_-wveW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "\n",
        "def run_one_fold(train_idx, val_idx, df, fold_id):\n",
        "    train_fold = df.iloc[train_idx].reset_index(drop=True)\n",
        "    val_fold   = df.iloc[val_idx].reset_index(drop=True)\n",
        "\n",
        "    train_ds = LettuceDataset(train_fold, TRAIN_RGB_DIR, TRAIN_DEPTH_DIR, train_mode=True, augment=True)\n",
        "    val_ds   = LettuceDataset(val_fold,   TRAIN_RGB_DIR, TRAIN_DEPTH_DIR, train_mode=True, augment=False)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=0)\n",
        "    val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "\n",
        "    model = SmallCNNMultiTask(dropout=0.3).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=8)\n",
        "\n",
        "    best_val = float(\"inf\")\n",
        "    best_state = None\n",
        "    patience = 20\n",
        "    patience_ctr = 0\n",
        "\n",
        "    for epoch in range(1, 151):\n",
        "        tr_loss, tr_main, tr_aux = train_one_epoch(model, train_loader)\n",
        "        val_mae = evaluate(model, val_loader)\n",
        "\n",
        "        scheduler.step(val_mae)\n",
        "\n",
        "        if val_mae < best_val - 1e-4:\n",
        "            best_val = val_mae\n",
        "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
        "            patience_ctr = 0\n",
        "        else:\n",
        "            patience_ctr += 1\n",
        "\n",
        "        if epoch % 20 == 0 or epoch == 1:\n",
        "            lr = optimizer.param_groups[0][\"lr\"]\n",
        "            print(f\"Fold {fold_id} | Epoch {epoch:03d} | lr={lr:.2e} | trainMAE={tr_main:.4f} | valMAE={val_mae:.4f}\")\n",
        "\n",
        "        if patience_ctr >= patience:\n",
        "            break\n",
        "\n",
        "    return best_val, best_state\n",
        "\n",
        "# Run CV\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "fold_maes = []\n",
        "fold_states = []  # store best weights per fold for later ensembling\n",
        "\n",
        "df = train_df.copy().reset_index(drop=True)\n",
        "\n",
        "for fold_id, (tr_idx, va_idx) in enumerate(kf.split(df), start=1):\n",
        "    print(\"\\n==============================\")\n",
        "    print(\"Starting fold\", fold_id)\n",
        "    print(\"==============================\")\n",
        "    best_val, best_state = run_one_fold(tr_idx, va_idx, df, fold_id)\n",
        "    fold_maes.append(best_val)\n",
        "    fold_states.append(best_state)\n",
        "    print(f\"Fold {fold_id} BEST val MAE: {best_val:.4f}\")\n",
        "\n",
        "print(\"\\n===== CV RESULTS =====\")\n",
        "print(\"Fold MAEs:\", [round(x,4) for x in fold_maes])\n",
        "print(\"Mean MAE:\", float(np.mean(fold_maes)))\n",
        "print(\"Std  MAE:\", float(np.std(fold_maes)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYjS17XEjJmd",
        "outputId": "b4c92b97-edd6-41cc-d8a1-19fc0ba9c5a7"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "Starting fold 1\n",
            "==============================\n",
            "Fold 1 | Epoch 001 | lr=1.00e-03 | trainMAE=5.6113 | valMAE=4.7024\n",
            "Fold 1 | Epoch 020 | lr=1.00e-03 | trainMAE=5.5989 | valMAE=4.5925\n",
            "Fold 1 BEST val MAE: 4.5876\n",
            "\n",
            "==============================\n",
            "Starting fold 2\n",
            "==============================\n",
            "Fold 2 | Epoch 001 | lr=1.00e-03 | trainMAE=5.8737 | valMAE=4.9856\n",
            "Fold 2 | Epoch 020 | lr=2.50e-04 | trainMAE=5.8850 | valMAE=5.0768\n",
            "Fold 2 BEST val MAE: 4.9856\n",
            "\n",
            "==============================\n",
            "Starting fold 3\n",
            "==============================\n",
            "Fold 3 | Epoch 001 | lr=1.00e-03 | trainMAE=5.5935 | valMAE=5.8447\n",
            "Fold 3 | Epoch 020 | lr=2.50e-04 | trainMAE=5.5842 | valMAE=6.1052\n",
            "Fold 3 BEST val MAE: 5.8447\n",
            "\n",
            "==============================\n",
            "Starting fold 4\n",
            "==============================\n",
            "Fold 4 | Epoch 001 | lr=1.00e-03 | trainMAE=5.3600 | valMAE=5.7097\n",
            "Fold 4 | Epoch 020 | lr=2.50e-04 | trainMAE=5.3513 | valMAE=5.7596\n",
            "Fold 4 BEST val MAE: 5.7097\n",
            "\n",
            "==============================\n",
            "Starting fold 5\n",
            "==============================\n",
            "Fold 5 | Epoch 001 | lr=1.00e-03 | trainMAE=5.4325 | valMAE=6.1565\n",
            "Fold 5 | Epoch 020 | lr=2.50e-04 | trainMAE=5.4287 | valMAE=6.2905\n",
            "Fold 5 BEST val MAE: 6.1565\n",
            "\n",
            "===== CV RESULTS =====\n",
            "Fold MAEs: [4.5876, 4.9856, 5.8447, 5.7097, 6.1565]\n",
            "Mean MAE: 5.456827953587408\n",
            "Std  MAE: 0.5798998143858589\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 6B.1"
      ],
      "metadata": {
        "id": "y4KTuh5QDqQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect one fold's targets distribution quickly\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "tr_idx, va_idx = next(iter(kf.split(df)))\n",
        "\n",
        "train_fold = df.iloc[tr_idx]\n",
        "val_fold = df.iloc[va_idx]\n",
        "\n",
        "print(\"Train fold DryWeightShoot stats:\")\n",
        "print(train_fold[\"DryWeightShoot\"].describe())\n",
        "\n",
        "print(\"\\nVal fold DryWeightShoot stats:\")\n",
        "print(val_fold[\"DryWeightShoot\"].describe())\n",
        "\n",
        "print(\"\\nAny NaNs?\")\n",
        "print(\"Train NaNs:\", train_fold[\"DryWeightShoot\"].isna().sum())\n",
        "print(\"Val NaNs  :\", val_fold[\"DryWeightShoot\"].isna().sum())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xg3K9hEOvhY3",
        "outputId": "5ad88a37-8b9a-4bc7-c537-f6b8a5f2482b"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train fold DryWeightShoot stats:\n",
            "count    184.000000\n",
            "mean       5.633804\n",
            "std        4.705485\n",
            "min        0.090000\n",
            "25%        1.697500\n",
            "50%        3.895000\n",
            "75%        9.332500\n",
            "max       18.210000\n",
            "Name: DryWeightShoot, dtype: float64\n",
            "\n",
            "Val fold DryWeightShoot stats:\n",
            "count    46.000000\n",
            "mean      4.632826\n",
            "std       3.882927\n",
            "min       0.110000\n",
            "25%       1.605000\n",
            "50%       3.900000\n",
            "75%       7.300000\n",
            "max      16.470000\n",
            "Name: DryWeightShoot, dtype: float64\n",
            "\n",
            "Any NaNs?\n",
            "Train NaNs: 0\n",
            "Val NaNs  : 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build fold loaders (small) and run 10 optimizer steps, print loss\n",
        "train_ds_dbg = LettuceDataset(train_fold.sample(32, random_state=0).reset_index(drop=True),\n",
        "                              TRAIN_RGB_DIR, TRAIN_DEPTH_DIR, train_mode=True, augment=True)\n",
        "train_loader_dbg = DataLoader(train_ds_dbg, batch_size=8, shuffle=True, num_workers=0)\n",
        "\n",
        "model_dbg = SmallCNNMultiTask(dropout=0.3).to(device)\n",
        "opt_dbg = optim.Adam(model_dbg.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "\n",
        "model_dbg.train()\n",
        "for step, (xb, yb, yauxb, ids) in enumerate(train_loader_dbg):\n",
        "    xb = xb.to(device).float()\n",
        "    yb = yb.to(device).float()\n",
        "    yauxb = yauxb.to(device).float()\n",
        "\n",
        "    opt_dbg.zero_grad()\n",
        "    pred_main, pred_aux = model_dbg(xb)\n",
        "\n",
        "    loss_main = mae(pred_main, yb)\n",
        "    loss_aux  = mse(pred_aux, yauxb)\n",
        "    loss = loss_main + 0.3 * loss_aux\n",
        "    loss.backward()\n",
        "    opt_dbg.step()\n",
        "\n",
        "    print(f\"step {step} | loss_main(MAE)={loss_main.item():.4f} | loss_aux(MSE)={loss_aux.item():.4f}\")\n",
        "    if step == 9:\n",
        "        break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sC7OhXJIDsB6",
        "outputId": "ac45b237-4dfa-4213-f375-3522f5e12c8a"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0 | loss_main(MAE)=2.7474 | loss_aux(MSE)=0.6958\n",
            "step 1 | loss_main(MAE)=2.9483 | loss_aux(MSE)=1.1962\n",
            "step 2 | loss_main(MAE)=3.4221 | loss_aux(MSE)=0.7710\n",
            "step 3 | loss_main(MAE)=3.7630 | loss_aux(MSE)=1.0145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 6B Fix"
      ],
      "metadata": {
        "id": "R4SM_Qu6ESV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "mae = nn.L1Loss()\n",
        "mse = nn.MSELoss()\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, device, lambda_aux=0.3):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_main = 0.0\n",
        "    total_aux  = 0.0\n",
        "    n = 0\n",
        "\n",
        "    for xb, yb, yauxb, ids in loader:\n",
        "        xb = xb.to(device).float()\n",
        "        yb = yb.to(device).float()\n",
        "        yauxb = yauxb.to(device).float()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        pred_main, pred_aux = model(xb)\n",
        "\n",
        "        loss_main = mae(pred_main, yb)\n",
        "        loss_aux  = mse(pred_aux, yauxb)\n",
        "        loss = loss_main + lambda_aux * loss_aux\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        bs = xb.size(0)\n",
        "        total_loss += loss.item() * bs\n",
        "        total_main += loss_main.item() * bs\n",
        "        total_aux  += loss_aux.item() * bs\n",
        "        n += bs\n",
        "\n",
        "    return total_loss / n, total_main / n, total_aux / n\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_mae(model, loader, device):\n",
        "    model.eval()\n",
        "    total = 0.0\n",
        "    n = 0\n",
        "    for xb, yb, yauxb, ids in loader:\n",
        "        xb = xb.to(device).float()\n",
        "        yb = yb.to(device).float()\n",
        "        pred_main, _ = model(xb)\n",
        "        loss_main = mae(pred_main, yb)\n",
        "        bs = xb.size(0)\n",
        "        total += loss_main.item() * bs\n",
        "        n += bs\n",
        "    return total / n\n"
      ],
      "metadata": {
        "id": "VqXcoWg9DuKi"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def run_one_fold(train_idx, val_idx, df, fold_id, lambda_aux=0.3):\n",
        "    train_fold = df.iloc[train_idx].reset_index(drop=True)\n",
        "    val_fold   = df.iloc[val_idx].reset_index(drop=True)\n",
        "\n",
        "    train_ds = LettuceDataset(train_fold, TRAIN_RGB_DIR, TRAIN_DEPTH_DIR, train_mode=True, augment=True)\n",
        "    val_ds   = LettuceDataset(val_fold,   TRAIN_RGB_DIR, TRAIN_DEPTH_DIR, train_mode=True, augment=False)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=0)\n",
        "    val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "\n",
        "    model = SmallCNNMultiTask(dropout=0.3).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=8)\n",
        "\n",
        "    best_val = float(\"inf\")\n",
        "    best_state = None\n",
        "    patience = 20\n",
        "    patience_ctr = 0\n",
        "\n",
        "    for epoch in range(1, 151):\n",
        "        tr_loss, tr_main, tr_aux = train_one_epoch(model, train_loader, optimizer, device, lambda_aux=lambda_aux)\n",
        "        val_mae = evaluate_mae(model, val_loader, device)\n",
        "\n",
        "        scheduler.step(val_mae)\n",
        "\n",
        "        if val_mae < best_val - 1e-4:\n",
        "            best_val = val_mae\n",
        "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
        "            patience_ctr = 0\n",
        "        else:\n",
        "            patience_ctr += 1\n",
        "\n",
        "        if epoch % 20 == 0 or epoch == 1:\n",
        "            lr = optimizer.param_groups[0][\"lr\"]\n",
        "            print(f\"Fold {fold_id} | Epoch {epoch:03d} | lr={lr:.2e} | trainMAE={tr_main:.4f} | valMAE={val_mae:.4f}\")\n",
        "\n",
        "        if patience_ctr >= patience:\n",
        "            break\n",
        "\n",
        "    return best_val, best_state\n",
        "\n",
        "# Run CV\n",
        "df = train_df.copy().reset_index(drop=True)\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "fold_maes = []\n",
        "fold_states = []\n",
        "\n",
        "for fold_id, (tr_idx, va_idx) in enumerate(kf.split(df), start=1):\n",
        "    print(\"\\n==============================\")\n",
        "    print(\"Starting fold\", fold_id)\n",
        "    print(\"==============================\")\n",
        "    best_val, best_state = run_one_fold(tr_idx, va_idx, df, fold_id, lambda_aux=0.3)\n",
        "    fold_maes.append(best_val)\n",
        "    fold_states.append(best_state)\n",
        "    print(f\"Fold {fold_id} BEST val MAE: {best_val:.4f}\")\n",
        "\n",
        "print(\"\\n===== CV RESULTS =====\")\n",
        "print(\"Fold MAEs:\", [round(x,4) for x in fold_maes])\n",
        "print(\"Mean MAE:\", float(np.mean(fold_maes)))\n",
        "print(\"Std  MAE:\", float(np.std(fold_maes)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rA_js_QNEUVE",
        "outputId": "d19aa605-dc78-41d1-9840-f5054a1315d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "Starting fold 1\n",
            "==============================\n",
            "Fold 1 | Epoch 001 | lr=1.00e-03 | trainMAE=3.8331 | valMAE=2.1016\n",
            "Fold 1 | Epoch 020 | lr=1.00e-03 | trainMAE=1.5415 | valMAE=0.8974\n",
            "Fold 1 | Epoch 040 | lr=5.00e-04 | trainMAE=1.2666 | valMAE=0.9016\n",
            "Fold 1 | Epoch 060 | lr=2.50e-04 | trainMAE=1.1720 | valMAE=0.6844\n",
            "Fold 1 BEST val MAE: 0.5918\n",
            "\n",
            "==============================\n",
            "Starting fold 2\n",
            "==============================\n",
            "Fold 2 | Epoch 001 | lr=1.00e-03 | trainMAE=3.4629 | valMAE=2.8508\n",
            "Fold 2 | Epoch 020 | lr=1.00e-03 | trainMAE=1.4392 | valMAE=0.8374\n",
            "Fold 2 | Epoch 040 | lr=5.00e-04 | trainMAE=1.0181 | valMAE=0.6906\n",
            "Fold 2 | Epoch 060 | lr=2.50e-04 | trainMAE=1.1130 | valMAE=0.6553\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bxcPDkweEWWC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}